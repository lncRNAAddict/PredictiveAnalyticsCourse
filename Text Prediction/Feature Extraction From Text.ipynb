{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction from Text data\n",
    "\n",
    "Often times, data is not structured (in the form of rows and columns). Text data is one of the most common types of unstructured data. Therefore, features need to extracted if feature-based models are to be used.\n",
    "\n",
    "\n",
    "In this tutorial, we will illustrate the feature extraction from text for classification purposes.\n",
    "\n",
    "We will first import the `SMS spam` dataset that contains Phone messages which are spam and some which are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "\n",
    "- More details can be found in Chapters 2 and 3 of `Natural Language Processing in Action` by `Hobson Lane, Cole Howard, Hannes Hapke`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import the csv file that contains the SMS spam data set.\n",
    "* There are two class labels `ham` (not spam) and `spam`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>K..give back my thanks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Am also doing in cbe only. But have to pay.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>complimentary 4 STAR Ibiza Holiday or £10,000 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>okmail: Dear Dave this is your final notice to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Aiya we discuss later lar... Pick u up at 4 is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5553</th>\n",
       "      <td>ham</td>\n",
       "      <td>You are a great role model. You are giving so ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5554</th>\n",
       "      <td>ham</td>\n",
       "      <td>Awesome, I remember the last time we got someb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5555</th>\n",
       "      <td>spam</td>\n",
       "      <td>If you don't, your prize will go to another cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5556</th>\n",
       "      <td>spam</td>\n",
       "      <td>SMS. ac JSco: Energy is high, but u may not kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5557</th>\n",
       "      <td>ham</td>\n",
       "      <td>Shall call now dear having food</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5558 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     class                                                sms\n",
       "0      ham                            K..give back my thanks.\n",
       "1      ham        Am also doing in cbe only. But have to pay.\n",
       "2     spam  complimentary 4 STAR Ibiza Holiday or £10,000 ...\n",
       "3     spam  okmail: Dear Dave this is your final notice to...\n",
       "4      ham  Aiya we discuss later lar... Pick u up at 4 is...\n",
       "...    ...                                                ...\n",
       "5553   ham  You are a great role model. You are giving so ...\n",
       "5554   ham  Awesome, I remember the last time we got someb...\n",
       "5555  spam  If you don't, your prize will go to another cu...\n",
       "5556  spam  SMS. ac JSco: Energy is high, but u may not kn...\n",
       "5557   ham                    Shall call now dear having food\n",
       "\n",
       "[5558 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_msgs = pd.read_csv('sms_spam.csv',names= ['class', 'sms'],header=1)\n",
    "sms_msgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many messages are spam and not-spam. \n",
    "\n",
    "* You can see here that the number of messages belonging to different classes are not equal. \n",
    "* There is a class imbalance here. This should be corrected ideally. But for now lets not worry about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4811\n",
       "spam     747\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_msgs['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ways to extract features from Text data. \n",
    "\n",
    "Two main ways to extract features from text data\n",
    "\n",
    "#### `Bags of words` or `Term Frequency` or `TF` : \n",
    "Vectors of word counts or frequencies. `term frequency or TF or bag of words` for a word is the number of times the word occurs in a text. `TF vector` is another way of calling `bag of words`.\n",
    "\n",
    "Below are three texts:\n",
    "\n",
    "* \"about the bird the bird bird bird bird\"\n",
    "* \"you heard about the bird\"\n",
    "* \"the bird is the word\"\n",
    "\n",
    "The `Term Freqeuncy matrix` of the three texts is:\n",
    "\n",
    "| about  | bird | heard | is | the | word | you |\n",
    "| ------ | ---- | ----- | -- | --- | ---- | --- |\n",
    "| 1      | 5    |  0    |  0 |  2  |  0   |   0 |\n",
    "| 1      | 1    |  1    |  0 |  1  |  0   |   1 |\n",
    "| 0      | 1    |  0    |  1 |  2  |  1   |   0 |\n",
    "\n",
    "The counts of each of seven words are the features. \n",
    "\n",
    "#### `TF-IDF vectors` or `term frequency times inverse document frequency`: \n",
    "\n",
    "This is a normalized version of `TF`. This one is used rather than Bag of Words because All Texts don't have same length and hence the `TF` should be normalized. More details on this later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Steps\n",
    "\n",
    "Text contains not just words but punctuations, abbreviations, lower case/ upper case etc. Therefore, Before feature extraction, we need to perform text preprocessing to clean the text data.\n",
    "\n",
    "There are three main steps of text preprocessing:\n",
    "\n",
    "#### Tokenization\n",
    "\n",
    "`tokenization` is a special case of sentence segmentation. Segmentation breaks up text into smaller chunks or segments. Each segment has some meaning.\n",
    "\n",
    "Tokenization focuses on segmenting text into tokens. Each token can be words or punctuation marks etc. depending on the problem context.\n",
    "\n",
    "- Lower casing all the words in the text.\n",
    "- Depending on the problem, Removing punctuations like . , ! \" or keep them.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Filtering \n",
    "- Removing Stop words\n",
    "- Removing anything unwanted words\n",
    "\n",
    "Stop words are common words in any language that occur in high frequency but carry much less meaning/information.\n",
    "\n",
    "- a, an, the, this, and, or, of, on\n",
    "\n",
    "You have a choice of removing these stop words after tokenization.\n",
    "\n",
    "#### Normalization\n",
    "\n",
    "\n",
    "- `case folding` : converting all letters/words in one specific case (lower or upper). Can also be a part of tokenization\n",
    "\n",
    "- `stemming` or `lemmatization`:  \n",
    "\n",
    "Lemmatization/Stemming is the process of converting a word to its base form. \n",
    "\n",
    "`flies` -> Lemmatization -> `fly`\n",
    "\n",
    "`flying` -> Lemmatization -> `fly`\n",
    "\n",
    "Without `Lemmatization/Stemming`, you will end up counting `flies` and `flying` as two different tokens/features.\n",
    "\n",
    "#### In our tutorial, we will skip `Normalization` because of lack of prerequisites from Natural Language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting `Bag of Words or TF ` using  `CountVectorizer` from `scikitlearn`\n",
    "\n",
    "More documentation here https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "Lets illustrate first by using two texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk package has inbuilt tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary \n",
      "=============================================\n",
      "['brought', 'fast', 'faster', 'fruit', 'got', 'harry', 'home', 'store', 'went']\n",
      "\n",
      "This is the TF vector of the document\n",
      "=============================================\n",
      "[[0 0 3 0 1 2 1 1 0]\n",
      " [1 1 0 1 0 1 1 1 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soibamb/miniforge3/envs/tf_m1/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "'''\n",
    "This function allows to convert text documents to a matrix of token counts.\n",
    "'''\n",
    "vectorizer1 = CountVectorizer(lowercase=True, # Convert all characters to lowercase.\n",
    "                             stop_words = 'english', # a built-in stop word list for English is used. all words in the stop_words are removed\n",
    "                             min_df=1) # ignore words/tokens that have a document frequency strictly lower than the given threshold. \n",
    "\n",
    "'''\n",
    "vectorizer works on a list of documents/Texts. Therefore we need to create a list that contains the texts\n",
    "'''\n",
    "sentence1 = \"\"\"The Faster Harry got to the store, the faster Harry, the faster, would get home.\"\"\"\n",
    "sentence2 = \"\"\"Harry went to the store fast and brought some fruit home\"\"\"\n",
    "\n",
    "docs=[]\n",
    "docs.append(sentence1)\n",
    "docs.append(sentence2)\n",
    "tf_vector = vectorizer1.fit_transform(docs)\n",
    "\n",
    "'''\n",
    "gives a list of the unique tokens (the vocabulary)\n",
    "'''\n",
    "print(\"vocabulary \\n=============================================\")\n",
    "vocab = vectorizer1.get_feature_names()\n",
    "print(vocab)\n",
    "\n",
    "'''\n",
    "This is the TF vector of the document\n",
    "'''\n",
    "print(\"\\nThis is the TF vector of the document\\n=============================================\")\n",
    "print(tf_vector.todense()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, `tf_vector` is the Term Frequency vector of the text. It is a sparse matrix and we can convert to dense matrix using `todense()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can represent the Term Frequency vector using Pandas DataFrame (in the form a table) to be more readable. There is one row (number of documents) and four columns (number of tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brought</th>\n",
       "      <th>fast</th>\n",
       "      <th>faster</th>\n",
       "      <th>fruit</th>\n",
       "      <th>got</th>\n",
       "      <th>harry</th>\n",
       "      <th>home</th>\n",
       "      <th>store</th>\n",
       "      <th>went</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sentence1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           brought  fast  faster  fruit  got  harry  home  store  went\n",
       "sentence1        0     0       3      0    1      2     1      1     0\n",
       "sentence2        1     1       0      1    0      1     1      1     1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(tf_vector.todense(),columns=vocab,index=['sentence1','sentence2'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `TFIDF` or `Term Frequency Times Inverse Document Frequency`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is that importance of a token in a specific text relative to other texts should depend on \n",
    "\n",
    "- the normalized frequency (`raw count / total no. of words`) of the token in the specific document\n",
    "- number of texts  containing the token\n",
    "\n",
    "`TFIDF` or `term frequency times inverse document frequency` of a word/term in a text quantifies the importance of that word in the document relative to the rest of the texts.  \n",
    "\n",
    "- `IDF (Inverse Document Frequency)` of a word = ratio of total number of documents to the number of documents containing the word. Usually `logarithm` of the ratio is used. \n",
    "\n",
    "- `TFIDF` or `term frequency times inverse document frequency` of a term/word in a document is simply multiplication of normalized frequency of the term in the document to `IDF` of the word.\n",
    "\n",
    "For a given word/term `t` in a given document, `d`, in a lists of texts, `D`\n",
    "- `normalized TF(t,d) or bag of words (t,d) = count(t)/count(d)`\n",
    "- `IDF(t,D) = log(no. of docs/no. of docs containing t) + 1`\n",
    "- `TFIDF(t,d) = normalized TF(t,d) * IDF(t,D)`\n",
    "\n",
    "The effect of adding `1` to the IDF in the equation above is that terms with zero `IDF`, i.e., terms that occur in all documents in a training set, will not be entirely ignored. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting `TFIDF ` using  `TfidVectorizer` from `scikitlearn`\n",
    "\n",
    "`TfidVectorizer` calculates the TFIDF matrix in a slightly different manner.\n",
    "\n",
    "- Instead of using normalized TF(t,d), it uses TF(t,d)\n",
    "- Default: if option `smooth_idf=True`: IDF(t,D) = log( (1+ no. of docs)/(1+ no. of docs containing t)) + 1 \n",
    "- If option `smooth_idf=False`: IDF(t,D) = log(no. of docs/no. of docs containing t) + 1 \n",
    "- TFIDF(t,d) = TF(t,d) * IDF(t,D) \n",
    "- Scikit learn reports normalized TFIDF(t,d)  as  TFIDF(t)/sqrt($\\sum_{w\\in d} TFIDF(w,d)^2$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets illustrate this using three simple texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Vocabulary of the documents\n",
      "\n",
      "['apple', 'boy', 'cat', 'dog', 'egg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soibamb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print(\"\\n---------------------------------------------------------------------------------\\n\")\n",
    "'''\n",
    "Three example texts\n",
    "'''\n",
    "test_docs =[\"apple boy cat to dog\",\"apple egg\",\"boy boy\"]\n",
    "vectorizer2 = TfidfVectorizer(lowercase=True, # Convert all characters to lowercase.\n",
    "                             stop_words = 'english', # a built-in stop word list for English is used. all words in the stop_words are removed\n",
    "                             min_df=1) # ignore words/tokens that have a document frequency strictly lower than the given threshold. \n",
    "model = vectorizer2.fit_transform(test_docs)\n",
    "tokens = vectorizer2.get_feature_names()\n",
    "print(\"Vocabulary of the documents\\n\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency times inverse document frequency as a Pandas DataFrame\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>apple</th>\n",
       "      <th>boy</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>egg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc1</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      apple   boy   cat   dog  egg\n",
       "doc1   0.43  0.43  0.56  0.56  0.0\n",
       "doc2   0.61  0.00  0.00  0.00  0.8\n",
       "doc3   0.00  1.00  0.00  0.00  0.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Term Frequency times inverse document frequency as a Pandas DataFrame\\n\")\n",
    "X = model.todense().round(2)\n",
    "X = pd.DataFrame(X,columns=tokens,index=['doc1','doc2','doc3'])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs =[\"apple boy cat to dog\",\"apple egg\",\"boy boy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term frequency matrix for these documents is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>apple</th>\n",
       "      <th>boy</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>egg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      apple  boy  cat  dog  egg\n",
       "doc1      1    1    1    1    0\n",
       "doc2      1    0    0    0    1\n",
       "doc3      0    2    0    0    0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y=pd.DataFrame(np.array([[1,1,1,1,0],\n",
    "                      [1,0,0,0,1],\n",
    "                      [0,2,0,0,0]]),columns=tokens,index=['doc1','doc2','doc3'])\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The feature `apple` has count 1 in doc1 and doc2. When normalized using `TFIDF`, `apple` has more importance in doc2 compared to doc1.\n",
    "- The feature `apple`, `boy`, `cat`, and `dog` have count = 1 in doc1. However, `apple` and `boy` are present in other texts: `doc2` & `doc3`. But `cat` and `dog` are only present in doc1. Therefore `cat` and `dog` are more important in doc1 when TFIDF is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn `TFIDF` matrix (not normalized is) for the first `doc` is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.28768207, 1.28768207, 1.69314718, 1.69314718, 0.        ])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "tmp1= np.array([1* (math.log((1+3)/(2+1)) + 1 ),\n",
    " 1* (math.log((1+3)/(2+1)) + 1),\n",
    " 1* (math.log((1+3)/(1+1)) + 1),\n",
    " 1* (math.log((1+3)/(1+1)) + 1),\n",
    " 0])\n",
    "tmp1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn TFIDF matrix (normalized) for the first `doc` is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.42804604, 0.42804604, 0.5628291 , 0.5628291 , 0.        ])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "tmp1/norm(tmp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other examples of using `TFIDFvectorizer` to extract features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using custom `stop_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer3 = TfidfVectorizer(lowercase=True, # Convert all characters to lowercase.\n",
    "                             stop_words = [\"all\",\"in\",\"the\",\"is\",\"and\"], # custom words to be ignored.\n",
    "                             min_df=2) # ignore terms that appeared in less than 2 texts  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing words that occur very frequently\n",
    "\n",
    "If there is a word that is contained in texts that belong to both the class labels, then the word may not contribute to differentiation between the two class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer4 = TfidfVectorizer(lowercase=True, # Convert all characters to lowercase.\n",
    "                             stop_words = [\"all\",\"in\",\"the\",\"is\",\"and\"], # custom words to be ignored.\n",
    "                             max_df=0.85) # ignore terms that appeared in 85% of the texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom tokenizer\n",
    "\n",
    "Below is a function that attempts to keep all punctuation, \n",
    "and special characters and separates words separared by tokens.\n",
    "\n",
    "For this, you should have some knowledge of regular expressions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def my_tokenizer(text):\n",
    "    # create a space between special characters \n",
    "    text=re.sub(\"(\\\\W)\",\" \\\\1 \",text)\n",
    "    # split based on whitespace\n",
    "    words = re.split(\"\\s+\",text)\n",
    "    words = [ w for w in words if w !=\"\"]\n",
    "    return words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soibamb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>,</th>\n",
       "      <th>.</th>\n",
       "      <th>brought</th>\n",
       "      <th>fast</th>\n",
       "      <th>faster</th>\n",
       "      <th>fruit</th>\n",
       "      <th>got</th>\n",
       "      <th>harry</th>\n",
       "      <th>home</th>\n",
       "      <th>store</th>\n",
       "      <th>went</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sentence1</th>\n",
       "      <td>0.625034</td>\n",
       "      <td>0.208345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.625034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.208345</td>\n",
       "      <td>0.296478</td>\n",
       "      <td>0.148239</td>\n",
       "      <td>0.148239</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425677</td>\n",
       "      <td>0.425677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302873</td>\n",
       "      <td>0.302873</td>\n",
       "      <td>0.302873</td>\n",
       "      <td>0.425677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ,         .   brought      fast    faster     fruit  \\\n",
       "sentence1  0.625034  0.208345  0.000000  0.000000  0.625034  0.000000   \n",
       "sentence2  0.000000  0.000000  0.425677  0.425677  0.000000  0.425677   \n",
       "\n",
       "                got     harry      home     store      went  \n",
       "sentence1  0.208345  0.296478  0.148239  0.148239  0.000000  \n",
       "sentence2  0.000000  0.302873  0.302873  0.302873  0.425677  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 = \"\"\"The Faster Harry got to the store, the faster Harry, the faster, would get home.\"\"\"\n",
    "sentence2 = \"\"\"Harry went to the store fast and brought some fruit home\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "docs=[]\n",
    "docs.append(sentence1)\n",
    "docs.append(sentence2)\n",
    "\n",
    "vectorizer5 = TfidfVectorizer(lowercase=True,\n",
    "                     tokenizer=my_tokenizer1, # use the tokenizer function defined\n",
    "                     stop_words='english',\n",
    "                     min_df = 1)\n",
    "tf_vector5 = vectorizer5.fit_transform(docs)\n",
    "df = pd.DataFrame(tf_vector5.todense(),columns=vectorizer5.get_feature_names(),index=['sentence1','sentence2'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limiting the size of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer6 = TfidfVectorizer(lowercase=True, # Convert all characters to lowercase.\n",
    "                              max_features = 10) # consider the top max_features ordered by term frequency across the texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word level – N-grams (unigrams and bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`N-grams`: a feature is a sequence of N consecutive words.\n",
    "\n",
    "Sometimes `bi-grams` and `tri-grams` may capture contextual information compared to just `unigrams`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nngram_range of (1, 1) means only unigrams, \\n(1, 2) means unigrams and bigrams, and \\n(2, 2) means only bigrams. \\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer7 = TfidfVectorizer(lowercase=True, \n",
    "                              ngram_range = (1,2))\n",
    "\n",
    "'''\n",
    "ngram_range of (1, 1) means only unigrams, \n",
    "(1, 2) means unigrams and bigrams, and \n",
    "(2, 2) means only bigrams. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features are \n",
      "\n",
      "['and', 'and brought', 'brought', 'brought some', 'fast', 'fast and', 'faster', 'faster harry', 'faster would', 'fruit', 'fruit home', 'get', 'get home', 'got', 'got to', 'harry', 'harry got', 'harry the', 'harry went', 'home', 'some', 'some fruit', 'store', 'store fast', 'store the', 'the', 'the faster', 'the store', 'to', 'to the', 'went', 'went to', 'would', 'would get']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soibamb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>and brought</th>\n",
       "      <th>brought</th>\n",
       "      <th>brought some</th>\n",
       "      <th>fast</th>\n",
       "      <th>fast and</th>\n",
       "      <th>faster</th>\n",
       "      <th>faster harry</th>\n",
       "      <th>faster would</th>\n",
       "      <th>fruit</th>\n",
       "      <th>...</th>\n",
       "      <th>store the</th>\n",
       "      <th>the</th>\n",
       "      <th>the faster</th>\n",
       "      <th>the store</th>\n",
       "      <th>to</th>\n",
       "      <th>to the</th>\n",
       "      <th>went</th>\n",
       "      <th>went to</th>\n",
       "      <th>would</th>\n",
       "      <th>would get</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sentence1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448932</td>\n",
       "      <td>0.299288</td>\n",
       "      <td>0.149644</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149644</td>\n",
       "      <td>0.425892</td>\n",
       "      <td>0.448932</td>\n",
       "      <td>0.106473</td>\n",
       "      <td>0.106473</td>\n",
       "      <td>0.106473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149644</td>\n",
       "      <td>0.149644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence2</th>\n",
       "      <td>0.238748</td>\n",
       "      <td>0.238748</td>\n",
       "      <td>0.238748</td>\n",
       "      <td>0.238748</td>\n",
       "      <td>0.238748</td>\n",
       "      <td>0.238748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.238748</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169871</td>\n",
       "      <td>0.169871</td>\n",
       "      <td>0.169871</td>\n",
       "      <td>0.238748</td>\n",
       "      <td>0.238748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                and  and brought   brought  brought some      fast  fast and  \\\n",
       "sentence1  0.000000     0.000000  0.000000      0.000000  0.000000  0.000000   \n",
       "sentence2  0.238748     0.238748  0.238748      0.238748  0.238748  0.238748   \n",
       "\n",
       "             faster  faster harry  faster would     fruit  ...  store the  \\\n",
       "sentence1  0.448932      0.299288      0.149644  0.000000  ...   0.149644   \n",
       "sentence2  0.000000      0.000000      0.000000  0.238748  ...   0.000000   \n",
       "\n",
       "                the  the faster  the store        to    to the      went  \\\n",
       "sentence1  0.425892    0.448932   0.106473  0.106473  0.106473  0.000000   \n",
       "sentence2  0.169871    0.000000   0.169871  0.169871  0.169871  0.238748   \n",
       "\n",
       "            went to     would  would get  \n",
       "sentence1  0.000000  0.149644   0.149644  \n",
       "sentence2  0.238748  0.000000   0.000000  \n",
       "\n",
       "[2 rows x 34 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 = \"\"\"The Faster Harry got to the store, the faster Harry, the faster, would get home.\"\"\"\n",
    "sentence2 = \"\"\"Harry went to the store fast and brought some fruit home\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "docs=[]\n",
    "docs.append(sentence1)\n",
    "docs.append(sentence2)\n",
    "\n",
    "\n",
    "tf_vector7 = vectorizer7.fit_transform(docs)\n",
    "print(\"features are \\n\")\n",
    "print(vectorizer7.get_feature_names())\n",
    "df = pd.DataFrame(tf_vector7.todense(),columns=vectorizer7.get_feature_names(),index=['sentence1','sentence2'])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features fom  SMS spam data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training and Testing samples\n",
      "\n",
      "(4446,)\n",
      "(1112,)\n",
      "(4446,)\n",
      "(1112,)\n",
      "Shape of Training data\n",
      "(4446, 7431)\n",
      "Shape of Testing data\n",
      "(1112, 7431)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Separate the texts & class labels\n",
    "'''\n",
    "sms_label = sms_msgs['class']\n",
    "sms_text  = sms_msgs['sms']\n",
    "\n",
    "'''\n",
    "Split train and test samples\n",
    "'''\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "msg_train, msg_test, label_train, label_test = train_test_split(sms_text, sms_label, test_size=0.2)\n",
    "\n",
    "print(\"Number of Training and Testing samples\\n\")\n",
    "print(msg_train.shape)\n",
    "print(msg_test.shape)\n",
    "print(label_train.shape)\n",
    "print(label_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "First fit the tfidfvectorizer on the training set to extract features\n",
    "'''\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "train_texts=msg_train.to_list() # list that contain the texts from training set\n",
    "vectorizer = TfidfVectorizer(lowercase=True, # Convert all characters to lowercase.\n",
    "                             stop_words = 'english', # a built-in stop word list for English is used. all words in the stop_words are removed\n",
    "                             min_df=1) \n",
    "sms_train_tfidf = vectorizer.fit_transform(train_texts)\n",
    "sms_train_tfidf = sms_train_tfidf.todense()\n",
    "print(\"Shape of Training data\")\n",
    "print(sms_train_tfidf.shape)\n",
    "\n",
    "\n",
    "'''\n",
    "Use the same  tfidfvectorizer to transform the testing set\n",
    "'''\n",
    "\n",
    "test_texts=msg_test.to_list()\n",
    "sms_test_tfidf = vectorizer.transform(test_texts)\n",
    "sms_test_tfidf = sms_test_tfidf.todense()\n",
    "print(\"Shape of Testing data\")\n",
    "print(sms_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features are\n",
      "['00', '000', '000pes', '008704050406', '0089', '01223585236', '0125698789', '02', '0207', '02072069400', '02073162414', '02085076972', '021', '03', '04', '05', '050703', '0578', '06', '07', '07008009200', '07046744435', '07090201529', '07090298926', '07099833605', '07123456789', '07732584351', '07734396839', '07742676969', '07753741225', '0776xxxxxxx', '07781482378', '07786200117', '077xxx', '07808247860', '07808726822', '07821230901', '078498', '07880867867', '0789xxxxxxx', '07946746291', '0796xxxxxx', '07973788240', '07xxxxxxxxx', '08', '0800', '08000407165', '08000776320', '08000839402', '08000930705', '08000938767', '08001950382', '08002888812', '08002986030', '08002986906', '08002988890', '08006344447', '0808', '08081263000', '08081560665', '0825', '083', '0844', '08448350055', '08448714184', '0845', '08450542832', '08452810071', '08452810073', '08452810075over18', '0870', '08700469649', '08700621170150p', '08701213186', '08701237397', '08701417012', '08701417012150p', '087018728737', '0870241182716', '08702490080', '08702840625', '08704050406', '08704439680', '08706091795', '0870737910216yrs', '08707500020', '08707509020', '0870753331018', '08707808226', '08708034412', '08708800282', '08709222922', '08709501522', '0871', '087104711148', '08712103738', '0871212025016', '08712300220', '087123002209am', '08712317606', '08712400602450p', '08712400603', '08712402779', '08712402902', '08712402972', '08712404000', '08712405020', '08712405022', '08712460324', '08712466669', '0871277810710p', '0871277810810', '0871277810910p', '08714342399', '087147123779am', '08714712379', '08714712394', '08714714011', '08715203028', '08715203649', '08715203652', '08715203656', '08715203677', '08715203685', '08715205273', '08715500022', '08715705022', '08717168528', '0871750', '08717507382', '08717509990', '08717890890', '08717895698', '08717898035', '08718711108', '08718720201', '08718726270', '087187262701', '08718726971', '08718726978', '087187272008', '08718727868', '08718727870', '08718727870150ppm', '08718730555', '08718730666', '08718738001', '08718738002', '08718738034', '08719180219', '08719180248', '08719181259', '08719181503', '08719839835', '08719899229', '08719899230', '09', '09050000301', '09050000332', '09050000460', '09050000555', '09050000878', '09050000928', '09050001295', '09050001808', '09050002311', '09050003091', '09050005321', '09050090044', '09056242159', '09057039994', '09058091854', '09058091870', '09058094454', '09058094455', '09058094507', '09058094565', '09058094597', '09058094599', '09058095107', '09058095201', '09058097218', '09058098002', '09058099801', '09061104276', '09061104283', '09061209465', '09061213237', '09061221061', '09061221066', '09061701444', '09061701461', '09061701851', '09061701939', '09061702893', '09061743386', '09061743806', '09061743811', '09061744553', '09061749602', '09061790121', '09061790125', '09061790126', '09063440451', '09063442151', '09063458130', '0906346330', '09064011000', '09064012103', '09064012160', '09064015307', '09064017305', '09064019014', '09064019788', '09065069154', '09065171142', '09065174042', '09065394514', '09065989180', '09065989182', '09066350750', '09066358152', '09066358361', '09066362206', '09066362231', '09066364311', '09066364349', '09066364589', '09066368327', '09066368470', '09066380611', '09066382422', '09066612661', '09066649731from', '09066660100', '09071512432', '09071512433', '09090204448', '09090900040', '09094100151', '09094646631', '09094646899', '09095350301', '09096102316', '09099726429', '09099726481', '09099726553', '09111032124', '09701213186', '10', '100', '1000', '1000call', '1000s', '100percent', '100txt', '1013', '1030', '10am', '10k', '10p', '10th', '11', '1120', '113', '1131', '114', '1146', '1172', '118p', '11mths', '11pm', '12', '1205', '121', '123', '1250', '125gift', '128', '12hours', '12hrs', '12mths', '13', '130', '1327', '139', '14', '145', '1450', '146tf150p', '14thmarch', '15', '150', '1500', '150p', '150p16', '150pm', '150ppermesssubscription', '150ppm', '150ppmsg', '151', '153', '15541', '16', '165', '169', '177', '18', '18p', '18yrs', '195', '1956669', '1apple', '1cup', '1da', '1er', '1hr', '1im', '1lemon', '1pm', '1st', '1st4terms', '1stchoice', '1stone', '1thing', '1tulsi', '1win150ppmx3', '1winaweek', '1winawk', '1x150p', '1yf', '20', '200', '2000', '2003', '2004', '2005', '2006', '2007', '200p', '20m12aq', '20p', '21', '21870000', '21st', '22', '220', '220cm2', '2309', '23f', '23g', '24', '24hrs', '24th', '25', '250', '250k', '255', '25p', '26', '2667', '26th', '27', '28', '28days', '28th', '28thfeb', '29', '2b', '2bold', '2channel', '2day', '2docd', '2exit', '2ez', '2find', '2getha', '2geva', '2go', '2gthr', '2hook', '2hrs', '2i', '2kbsubject', '2lands', '2marrow', '2moro', '2morow', '2morro', '2morrow', '2morrowxxxx', '2mro', '2mrw', '2mwen', '2nd', '2nights', '2nite', '2optout', '2p', '2price', '2px', '2rcv', '2stop', '2stoptx', '2stoptxt', '2u', '2u2', '2watershd', '2waxsto', '2wks', '2wt', '2wu', '2yr', '30', '300', '300603', '300603t', '300p', '3030', '30ish', '30th', '31', '3100', '31p', '32', '32000', '3230', '32323', '326', '33', '330', '350', '3510i', '35p', '3650', '36504', '3680', '373', '38', '382', '391784', '3aj', '3d', '3days', '3g', '3gbp', '3hrs', '3lions', '3lp', '3miles', '3mins', '3mobile', '3pound', '3qxj9', '3rd', '3ss', '3uz', '3wks', '3xx', '40', '400', '400mins', '400thousad', '402', '4041', '40411', '40533', '40gb', '40mph', '41685', '420', '42049', '4217', '42478', '42810', '430', '434', '44', '440', '4403ldnw1a7rw18', '447797706009', '447801259231', '448712404000', '449050000301', '449071512431', '450', '450ppw', '450pw', '45239', '45pm', '47', '4719', '4742', '48', '4882', '48922', '49', '49557', '4a', '4brekkie', '4d', '4eva', '4few', '4fil', '4get', '4give', '4got', '4goten', '4info', '4jx', '4msgs', '4mths', '4my', '4qf2', '4t', '4th', '4the', '4txt', '4u', '4utxt', '4w', '4ward', '4wrd', '4xx26', '4years', '50', '500', '5000', '50ea', '50gbp', '50p', '50perweeksub', '50perwksub', '50pm', '50pmmorefrommobile2bremoved', '50rcvd', '5226', '523', '5249', '526', '528', '530', '54', '542', '5ish', '5k', '5min', '5mls', '5p', '5pm', '5th', '5wb', '5we', '5wkg', '5wq', '5years', '60', '600', '6031', '6089', '60p', '61', '61200', '61610', '62220cncl', '62468', '62735', '630', '63miles', '65', '66', '6669', '674', '67441233', '68866', '69101', '69200', '69669', '69696', '69698', '69876', '69888', '69888nyt', '69911', '69969', '69988', '6days', '6hl', '6ish', '6missed', '6months', '6ph', '6pm', '6th', '6times', '6wu', '6zf', '700', '71', '7250', '7250i', '730', '731', '74355', '75', '750', '7548', '75max', '762', '7634', '7684', '77', '7732584351', '78', '786', '7876150ppm', '7am', '7ish', '7mp', '7pm', '7th', '7ws', '7zs', '80', '800', '8000930705', '80062', '8007', '80082', '80086', '80122300p', '80155', '80182', '80488', '80608', '8077', '80878', '81010', '81151', '81303', '81618', '82050', '82242', '82277', '82468', '83021', '83039', '83049', '83118', '83222', '83332', '83338', '83355', '83370', '83383', '83435', '83600', '83738', '84025', '84122', '84128', '84199', '84484', '85', '85023', '85069', '85233', '85555', '86021', '861', '864233', '86688', '86888', '87021', '87066', '87070', '87077', '87121', '87131', '8714714', '872', '87239', '87575', '8800', '88039', '88066', '88088', '88222', '88800', '8883', '88877', '88888', '89034', '89070', '89080', '89105', '89123', '89545', '89555', '89693', '89938', '8am', '8ball', '8p', '8pm', '8th', '8wp', '900', '9061100010', '9153', '9280114', '92h', '930', '9307622', '945', '946', '95', '9755', '9758', '98321561', '99', '9996', '9ae', '9am', '9ja', '9pm', '9t', '9th', '____', 'a21', 'a30', 'aa', 'aah', 'aathi', 'abdomen', 'abel', 'aberdeen', 'abi', 'ability', 'abiola', 'abj', 'able', 'abnormally', 'aboutas', 'abroad', 'absence', 'absolutly', 'abt', 'abta', 'aburo', 'abuse', 'ac', 'academic', 'acc', 'accent', 'accenture', 'accept', 'access', 'accessible', 'accidant', 'accident', 'accidentally', 'accommodationvouchers', 'accomodate', 'accomodations', 'accordingly', 'account', 'accounting', 'accounts', 'accumulation', 'achan', 'ache', 'achieve', 'acid', 'acknowledgement', 'acl03530150pm', 'acnt', 'aco', 'act', 'acted', 'actin', 'acting', 'action', 'activ8', 'activate', 'active', 'activities', 'actor', 'actual', 'actually', 'ad', 'adam', 'add', 'addamsfa', 'added', 'addicted', 'addie', 'adding', 'address', 'adds', 'adewale', 'admin', 'administrator', 'admirer', 'admission', 'admit', 'adore', 'adoring', 'adp', 'adress', 'adrian', 'adrink', 'ads', 'adult', 'adults', 'advance', 'adventure', 'adventuring', 'advice', 'advise', 'advising', 'advisors', 'aeronautics', 'aeroplane', 'affair', 'affairs', 'affection', 'affections', 'affidavit', 'afford', 'afraid', 'africa', 'african', 'aft', 'afternon', 'afternoon', 'afternoons', 'aftr', 'agalla', 'age', 'age16', 'agency', 'agent', 'agents', 'ages', 'aging', 'ago', 'agree', 'ah', 'aha', 'ahead', 'ahhh', 'ahhhh', 'ahmad', 'ahold', 'aid', 'aids', 'aig', 'aight', 'ain', 'aint', 'air', 'air1', 'airport', 'airtel', 'aiya', 'aiyah', 'aiyar', 'aiyo', 'ajith', 'ak', 'aka', 'akon', 'al', 'alaikkum', 'alaipayuthe', 'albi', 'album', 'alcohol', 'aldrine', 'alert', 'alertfrom', 'alerts', 'alex', 'alfie', 'algarve', 'algebra', 'algorithms', 'ali', 'alian', 'alibi', 'alive', 'allah', 'allo', 'allow', 'allowed', 'alright', 'alrite', 'alternative', 'aluable', 'alwa', 'alwys', 'amanda', 'amazing', 'ambitious', 'ambrith', 'american', 'ami', 'amigos', 'amk', 'amla', 'amma', 'ammae', 'ammo', 'amnow', 'amore', 'amplikater', 'amrita', 'ams', 'amt', 'amused', 'amy', 'ana', 'anal', 'anand', 'anderson', 'andre', 'andres', 'andrews', 'andros', 'angry', 'animal', 'animation', 'anjie', 'anjola', 'anna', 'annie', 'anniversary', 'annoncement', 'announced', 'announcement', 'annoyin', 'annoying', 'anonymous', 'anot', 'ans', 'ansr', 'answer', 'answered', 'answerin', 'answering', 'answers', 'answr', 'antelope', 'antha', 'anthony', 'anti', 'antibiotic', 'anybody', 'anymore', 'anyones', 'anyplaces', 'anythin', 'anythingtomorrow', 'anytime', 'anyways', 'aom', 'apart', 'apartment', 'apes', 'aphex', 'apnt', 'apo', 'apologetic', 'apologise', 'apologize', 'apology', 'app', 'apparently', 'appear', 'applausestore', 'applebees', 'apples', 'application', 'apply', 'applyed', 'applying', 'appointment', 'appointments', 'appreciate', 'appreciated', 'approaches', 'approaching', 'appropriate', 'approve', 'approx', 'apps', 'appt', 'april', 'aproach', 'apt', 'aquarius', 'ar', 'arab', 'arabian', 'arcade', 'archive', 'ard', 'area', 'aren', 'arent', 'arestaurant', 'aretaking', 'areyouunique', 'argentina', 'argh', 'argue', 'arguing', 'argument', 'arguments', 'aries', 'arise', 'arithmetic', 'arm', 'armand', 'armenia', 'arms', 'arng', 'arngd', 'arnt', 'aroundn', 'arr', 'arrange', 'arranging', 'arrested', 'arrival', 'arrive', 'arrived', 'arrow', 'arsenal', 'art', 'artists', 'arts', 'arty', 'arul', 'arun', 'asa', 'asap', 'asda', 'ashley', 'ashwini', 'asia', 'asian', 'asjesus', 'ask', 'askd', 'asked', 'askin', 'asking', 'asks', 'aslamalaikkum', 'asleep', 'asp', 'aspects', 'assessment', 'assistance', 'assume', 'assumed', 'asthere', 'asthma', 'astne', 'astoundingly', 'astrology', 'astronomer', 'asusual', 'ate', 'atlanta', 'atlast', 'atleast', 'atm', 'atrocious', 'attach', 'attached', 'attack', 'attempt', 'attend', 'attended', 'attention', 'attitude', 'attraction', 'attractive', 'attracts', 'attributed', 'atural', 'auction', 'audiitions', 'audition', 'audrey', 'audrie', 'august', 'aunt', 'auntie', 'aunties', 'aunts', 'aunty', 'australia', 'authorise', 'auto', 'autocorrect', 'av', 'ava', 'availa', 'available', 'avalarr', 'avatar', 'avble', 'ave', 'avenge', 'avent', 'avin', 'avo', 'avoid', 'avoiding', 'avoids', 'await', 'awaiting', 'awake', 'award', 'awarded', 'away', 'awesome', 'awkward', 'aww', 'awww', 'ax', 'axel', 'axis', 'ay', 'ayn', 'b4', 'b4190604', 'b4280703', 'b4u', 'b4utele', 'ba', 'ba128nnfwfly150ppm', 'baaaaaaaabe', 'baaaaabe', 'babe', 'babes', 'babies', 'baby', 'babygoodbye', 'babyjontet', 'babysit', 'babysitting', 'bac', 'backwards', 'bad', 'badly', 'badrith', 'bag', 'bags', 'bahamas', 'baig', 'bailiff', 'bajarangabali', 'bak', 'bakrid', 'balance', 'baller', 'balloon', 'balls', 'bam', 'bambling', 'band', 'bang', 'bangb', 'bangbabes', 'bani', 'bank', 'banks', 'banned', 'banneduk', 'banter', 'bar', 'barbie', 'barcelona', 'bare', 'barely', 'barkleys', 'barmed', 'barolla', 'barred', 'barring', 'barry', 'bars', 'base', 'based', 'basic', 'basically', 'basket', 'basketball', 'bat', 'batch', 'batchlor', 'bath', 'bathe', 'bathing', 'batsman', 'batt', 'battery', 'battle', 'bawling', 'bay', 'bb', 'bbd', 'bbdeluxe', 'bbq', 'bc', 'bcaz', 'bck', 'bcm1896wc1n3xx', 'bcm4284', 'bcmsfwc1n3xx', 'bcoz', 'bcs', 'bcum', 'bcums', 'bday', 'beach', 'beads', 'bears', 'beatings', 'beauties', 'beautiful', 'beauty', 'bec', 'becausethey', 'becoz', 'becz', 'bed', 'bedbut', 'bedreal', 'bedrm', 'bedroom', 'beeen', 'beehoon', 'beendropping', 'beer', 'beerage', 'beers', 'befor', 'beg', 'beggar', 'begging', 'begin', 'begins', 'begun', 'behalf', 'behave', 'bein', 'believe', 'belive', 'bell', 'bellearlier', 'belligerent', 'belly', 'belong', 'belongs', 'belovd', 'beloved', 'belt', 'ben', 'bend', 'beneath', 'beneficiary', 'benefits', 'bergkamp', 'best', 'best1', 'bet', 'beta', 'beth', 'betta', 'better', 'bettersn', 'bevies', 'beware', 'bf', 'bffs', 'bfore', 'bhaskar', 'bhayandar', 'bian', 'biatch', 'bid', 'bids', 'big', 'bigger', 'biggest', 'bike', 'billed', 'billion', 'billy', 'bilo', 'bimbo', 'bin', 'biola', 'bird', 'birds', 'birla', 'birth', 'birthdate', 'birthday', 'bishan', 'bit', 'bite', 'bites', 'bits', 'biz', 'bk', 'black', 'blackberry', 'blake', 'blame', 'blank', 'blanked', 'blanket', 'blankets', 'blastin', 'bleak', 'bleh', 'bless', 'blessed', 'blessing', 'blessings', 'blimey', 'blind', 'block', 'blocked', 'blog', 'bloke', 'blokes', 'bloo', 'blood', 'bloody', 'bloomberg', 'blow', 'blowing', 'blown', 'blu', 'blue', 'bluetooth', 'bluetoothhdset', 'bluff', 'blur', 'bmw', 'board', 'boat', 'boatin', 'bob', 'body', 'boggy', 'bognor', 'bold', 'bold2', 'bollox', 'boltblue', 'bone', 'bonus', 'boo', 'book', 'booked', 'bookedthe', 'booking', 'bookmark', 'books', 'bookshelf', 'boooo', 'boost', 'booty', 'bootydelious', 'borderline', 'bored', 'borin', 'boring', 'born', 'borrow', 'boss', 'boston', 'bot', 'bother', 'bothering', 'bottle', 'bought', 'bout', 'bowa', 'bowl', 'bowls', 'box', 'box1146', 'box139', 'box177', 'box245c2150pm', 'box326', 'box334', 'box334sk38ch', 'box385', 'box39822', 'box420', 'box42wr29c', 'box434sk38wp150ppm18', 'box61', 'box95qu', 'box97n7qp', 'boy', 'boye', 'boyf', 'boyfriend', 'boys', 'boytoy', 'bpo', 'brah', 'brain', 'braindance', 'brainless', 'brains', 'brainy', 'brand', 'brandy', 'brats', 'braved', 'bray', 'brb', 'brdget', 'bread', 'breadstick', 'break', 'breaker', 'breakfast', 'breakin', 'breaking', 'breath', 'breathe', 'breathe1', 'breather', 'breathing', 'breeze', 'breezy', 'bribe', 'brick', 'bridge', 'bridgwater', 'brief', 'bright', 'brighten', 'brilliant', 'brin', 'bring', 'bringing', 'brings', 'brison', 'bristol', 'british', 'britney', 'bro', 'broad', 'broadband', 'broke', 'broken', 'brolly', 'bros', 'broth', 'brothas', 'brother', 'brought', 'brown', 'brownies', 'browse', 'browser', 'browsin', 'bruce', 'brum', 'bruv', 'bslvyl', 'bsn', 'bstfrnd', 'bt', 'bthere', 'btw', 'btwn', 'bucks', 'bud', 'buddy', 'budget', 'buen', 'buff', 'buffet', 'buffy', 'bugis', 'build', 'building', 'built', 'bull', 'bunch', 'bundle', 'buns', 'burden', 'burger', 'burgundy', 'burial', 'burn', 'burning', 'burns', 'burnt', 'burrito', 'bus', 'bus8', 'buses', 'busetop', 'business', 'busty', 'busy', 'butt', 'buttheres', 'buy', 'buyer', 'buyers', 'buying', 'buz', 'buzy', 'buzz', 'bw', 'bx', 'bx420', 'bye', 'c52', 'cab', 'cabin', 'cable', 'cafe', 'cake', 'cakes', 'cal', 'calculation', 'cali', 'calicut', 'california', 'call09050000327', 'call2optout', 'callback', 'callcost', 'callcost150ppmmobilesvary', 'calld', 'called', 'caller', 'callers', 'callertune', 'callfreefone', 'callin', 'calling', 'callon', 'calls', 'calm', 'cam', 'camcorder', 'came', 'camera', 'campus', 'camry', 'canada', 'canal', 'canary', 'cancel', 'canceled', 'cancelled', 'cancer', 'canlove', 'cann', 'canname', 'cantdo', 'canteen', 'capacity', 'capital', 'cappuccino', 'caps', 'captain', 'captaining', 'car', 'card', 'cardiff', 'cards', 'care', 'careabout', 'cared', 'career', 'careers', 'careful', 'carefully', 'careless', 'cares', 'caring', 'carlie', 'carlin', 'carlos', 'carly', 'carolina', 'caroline', 'carry', 'carryin', 'cars', 'cartons', 'cartoon', 'case', 'cash', 'cashbin', 'cashed', 'cashto', 'casing', 'cast', 'casting', 'castor', 'casualty', 'cat', 'catch', 'catches', 'catching', 'caught', 'cause', 'causing', 'cave', 'caveboy', 'cbe', 'cc', 'cc100p', 'cd', 'cdgt', 'cds', 'ceiling', 'celeb', 'celebrate', 'celebrated', 'celebration', 'celebrations', 'cell', 'center', 'centre', 'century', 'cer', 'cereals', 'ceri', 'certainly', 'cha', 'chachi', 'chad', 'chain', 'challenge', 'champ', 'champlaxigating', 'champneys', 'chance', 'chances', 'change', 'changed', 'changes', 'changing', 'channel', 'chapel', 'chaps', 'chapter', 'character', 'characters', 'charge', 'charged', 'charges', 'charity', 'charles', 'charlie', 'charming', 'chart', 'chase', 'chasing', 'chastity', 'chat', 'chat80155', 'chatlines', 'chatter', 'chatting', 'cheap', 'cheaper', 'cheat', 'cheating', 'chechi', 'check', 'checked', 'checkin', 'checking', 'checkmate', 'checkup', 'cheer', 'cheered', 'cheers', 'cheery', 'cheese', 'cheesy', 'chennai', 'cherish', 'cherthala', 'chess', 'chest', 'chex', 'chez', 'chg', 'chgs', 'chic', 'chicken', 'chickened', 'chief', 'chikku', 'child', 'childish', 'childporn', 'children', 'childrens', 'chile', 'chill', 'chillaxin', 'chillin', 'china', 'chinatown', 'chinese', 'chinky', 'chiong', 'chip', 'chit', 'chk', 'chloe', 'chocolate', 'choice', 'choices', 'choose', 'choosing', 'chop', 'chores', 'chosen', 'chrgd', 'christ', 'christians', 'christmas', 'christmassy', 'chuck', 'chuckin', 'church', 'ciao', 'cine', 'cinema', 'citizen', 'city', 'citylink', 'cl', 'claim', 'claimcode', 'claims', 'claire', 'clarification', 'clarify', 'clark', 'clas', 'clash', 'class', 'classes', 'classic', 'classmates', 'claypot', 'cld', 'clean', 'cleaning', 'clear', 'cleared', 'clearer', 'clearing', 'clearly', 'clever', 'click', 'cliff', 'cliffs', 'clip', 'clock', 'clocks', 'clos1', 'close', 'closeby', 'closed', 'closer', 'closes', 'closingdate04', 'clothes', 'cloud', 'clover', 'club', 'club4', 'club4mobiles', 'clubmoby', 'clubsaisai', 'clue', 'cm', 'cm2', 'cme', 'cmon', 'cn', 'cnl', 'cnupdates', 'coach', 'coast', 'coat', 'coca', 'coccooning', 'cochin', 'coco', 'code', 'coffee', 'coherently', 'coin', 'coincidence', 'coins', 'cola', 'cold', 'colin', 'collages', 'collapsed', 'colleagues', 'collect', 'collected', 'collecting', 'collection', 'colleg', 'college', 'color', 'colour', 'colourful', 'colours', 'com', 'com1win150ppmx3age16', 'com1win150ppmx3age16subscription', 'comb', 'combination', 'combine', 'come', 'comedy', 'comes', 'comfey', 'comfort', 'comin', 'coming', 'comingdown', 'command', 'comment', 'commercial', 'common', 'community', 'como', 'comp', 'company', 'compare', 'compass', 'competition', 'complacent', 'complaining', 'complaint', 'complementary', 'complete', 'completed', 'completely', 'completes', 'completing', 'complimentary', 'comprehensive', 'compromised', 'compulsory', 'computational', 'computer', 'computerless', 'computers', 'comuk', 'concentrate', 'concentrating', 'concentration', 'concern', 'concerned', 'concert', 'conclusion', 'condition', 'conditions', 'conducts', 'conected', 'conference', 'confidence', 'configure', 'confirm', 'confirmed', 'confused', 'confuses', 'congrats', 'congratulation', 'congratulations', 'connect', 'connected', 'connection', 'connections', 'cons', 'consensus', 'conserve', 'consider', 'considering', 'consistently', 'console', 'constant', 'constantly', 'contact', 'contacted', 'contacts', 'content', 'contented', 'contention', 'contents', 'continue', 'continued', 'contract', 'contribute', 'control', 'convenience', 'converter', 'convey', 'convince', 'convincing', 'cook', 'cooked', 'cookies', 'cooking', 'cool', 'cooped', 'copied', 'copies', 'cops', 'copy', 'cornwall', 'corporation', 'corrct', 'correct', 'correction', 'correctly', 'corrupt', 'corvettes', 'cos', 'cosign', 'cost', 'costa', 'costing', 'costs', 'costume', 'costumes', 'cough', 'coughing', 'coulda', 'couldn', 'countin', 'country', 'counts', 'coupla', 'couple', 'course', 'court', 'courtroom', 'cousin', 'cover', 'coveragd', 'covers', 'coz', 'cozy', 'cps', 'cr', 'cr01327bt', 'cr9', 'crab', 'crack', 'craigslist', 'crammed', 'cramps', 'crap', 'crash', 'crashed', 'crashing', 'crave', 'craving', 'craziest', 'crazy', 'crazyin', 'cream', 'created', 'credit', 'credited', 'credits', 'creep', 'creepy', 'cres', 'cribbs', 'cricket', 'crickiting', 'cried', 'crisis', 'crore', 'cross', 'crossing', 'crowd', 'croydon', 'crucial', 'crucify', 'cruel', 'cruise', 'cruisin', 'crushes', 'cs', 'csbcm4235wc1n3xx', 'csc', 'csh11', 'cst', 'cstore', 'ctagg', 'ctargg', 'cthen', 'ctla', 'cts', 'cttargg', 'ctter', 'cttergg', 'ctxt', 'cu', 'cud', 'cuddle', 'cuddled', 'cuddling', 'cudnt', 'culdnt', 'cultures', 'cum', 'cup', 'cupboard', 'cuppa', 'curfew', 'curious', 'current', 'currently', 'curry', 'curtsey', 'cust', 'custcare', 'custom', 'customer', 'customercare', 'customers', 'customersqueries', 'cut', 'cute', 'cutefrnd', 'cutest', 'cutter', 'cutting', 'cuz', 'cw25wx', 'cya', 'cyclists', 'cysts', 'd3wv', 'da', 'daaaaa', 'dabbles', 'dad', 'daddy', 'dai', 'daily', 'dammit', 'damn', 'dan', 'dance', 'dancin', 'dancing', 'dane', 'dang', 'danger', 'dangerous', 'dao', 'dare', 'dark', 'darker', 'darkest', 'darkness', 'darlin', 'darling', 'darlings', 'darren', 'dartboard', 'dasara', 'dat', 'date', 'datebox1282essexcm61xn', 'dates', 'dating', 'dats', 'datz', 'dave', 'dawns', 'day', 'days', 'daytime', 'daywith', 'db', 'dbuk', 'dd', 'dead', 'deal', 'dealer', 'dealing', 'deals', 'dear', 'dear1', 'dearer', 'dearly', 'death', 'debating', 'dec', 'decades', 'december', 'decent', 'decide', 'decided', 'deciding', 'decimal', 'decision', 'deck', 'decking', 'declare', 'decorating', 'dedicate', 'dedicated', 'deduct', 'deep', 'deepak', 'deepest', 'deer', 'deeraj', 'def', 'defer', 'deficient', 'definite', 'definitely', 'degree', 'degrees', 'dehydrated', 'dehydration', 'del', 'delay', 'delayed', 'delete', 'deleted', 'delhi', 'delicious', 'deliver', 'delivered', 'deliveredtomorrow', 'delivery', 'deltomorrow', 'deluxe', 'dem', 'demand', 'den', 'dena', 'dengra', 'denis', 'dental', 'dentists', 'deny', 'department', 'dependents', 'depends', 'deposit', 'deposited', 'depressed', 'dept', 'der', 'derek', 'description', 'desert', 'deserve', 'desires', 'desk', 'desparate', 'desparately', 'desperate', 'despite', 'dessert', 'destination', 'destiny', 'details', 'determine', 'detroit', 'deus', 'develop', 'developed', 'device', 'devils', 'devouring', 'dey', 'dha', 'dhanush', 'dhina', 'dhoni', 'dhorte', 'di', 'dial', 'dialling', 'dialogue', 'diamond', 'diamonds', 'diapers', 'dice', 'dick', 'dict', 'dictionary', 'did', 'diddy', 'didn', 'didnt', 'didntgive', 'die', 'died', 'diesel', 'diet', 'dieting', 'diff', 'differ', 'difference', 'differences', 'different', 'difficult', 'difficulties', 'dificult', 'digi', 'digital', 'digits', 'dignity', 'dileep', 'dimension', 'din', 'dine', 'dined', 'dinero', 'ding', 'dining', 'dinner', 'dino', 'dint', 'dip', 'dippeditinadew', 'direct', 'directly', 'directors', 'dirt', 'dirtiest', 'dirty', 'dis', 'disagreeable', 'disappeared', 'disappointment', 'disaster', 'disasters', 'disastrous', 'disc', 'disclose', 'disconnected', 'discount', 'discreet', 'discuss', 'discussed', 'diseases', 'disk', 'dislikes', 'dismay', 'dismissial', 'display', 'distance', 'distract', 'disturb', 'disturbance', 'disturbing', 'ditto', 'divert', 'division', 'divorce', 'diwali', 'dizzamn', 'dizzee', 'dl', 'dled', 'dlf', 'dload', 'dnt', 'dob', 'dobby', 'doc', 'dock', 'docks', 'docs', 'doctor', 'doctors', 'documents', 'dodda', 'does', 'doesn', 'doesnt', 'dog', 'dogbreath', 'dogg', 'doggin', 'dogging', 'doggy', 'dogs', 'dogwood', 'doin', 'doinat', 'doing', 'doit', 'dokey', 'doll', 'dollar', 'dollars', 'dolls', 'dom', 'domain', 'don', 'donate', 'donno', 'dont', 'dontcha', 'dontignore', 'dontmatter', 'dontplease', 'donyt', 'door', 'doors', 'dorm', 'dormitory', 'dorothy', 'dose', 'dosomething', 'dot', 'double', 'doublemins', 'doubles', 'doubletxt', 'doubt', 'doug', 'dough', 'download', 'downloaded', 'downloads', 'downon', 'downstem', 'dozens', 'dr', 'dracula', 'drama', 'dramastorm', 'dramatic', 'drastic', 'draw', 'draws', 'dreading', 'dream', 'dreams', 'dreamz', 'dress', 'dressed', 'dresser', 'drink', 'drinkin', 'drinking', 'drinks', 'drive', 'drivin', 'driving', 'drms', 'drop', 'dropped', 'drops', 'drove', 'drpd', 'drug', 'drugs', 'drunk', 'drunkard', 'drunken', 'drvgsto', 'dry', 'dryer', 'dsn', 'dt', 'dual', 'dub', 'dubsack', 'duchess', 'dude', 'dudes', 'dudette', 'duffer', 'dull', 'dumb', 'dump', 'dun', 'dungerees', 'dunno', 'duo', 'durban', 'durham', 'dusk', 'dust', 'duvet', 'dvd', 'dvg', 'dwn', 'dying', 'dysentry', 'eachother', 'ear', 'earlier', 'early', 'earn', 'earning', 'ears', 'earth', 'easier', 'easiest', 'easily', 'east', 'eastenders', 'easter', 'easy', 'eat', 'eaten', 'eatin', 'eating', 'ebay', 'ec2a', 'echo', 'eckankar', 'ecstacy', 'ecstasy', 'ed', 'edge', 'edhae', 'edison', 'edition', 'edu', 'education', 'educational', 'edukkukayee', 'edward', 'edwards', 'ee', 'eek', 'eerie', 'eerulli', 'effect', 'effects', 'efficient', 'efreefone', 'egbon', 'egg', 'eggs', 'ego', 'eh', 'eh74rr', 'eighth', 'eire', 'el', 'ela', 'elaborate', 'elaborating', 'elaine', 'elama', 'eldest', 'election', 'elections', 'electricity', 'elliot', 'ello', 'elvis', 'em', 'email', 'embarassed', 'embarassing', 'embarrassed', 'embassy', 'emc1', 'emerging', 'emily', 'emotion', 'employee', 'employer', 'en', 'end', 'ended', 'ending', 'endof', 'endowed', 'ends', 'enemy', 'energy', 'eng', 'engaged', 'engalnd', 'engin', 'england', 'english', 'enjoy', 'enjoyed', 'enjoyin', 'enjoying', 'enketa', 'enna', 'ente', 'enter', 'entered', 'enters', 'entertain', 'entire', 'entirely', 'entitled', 'entrepreneurs', 'entropication', 'entry', 'entry41', 'enufcredeit', 'enuff', 'envelope', 'environment', 'envy', 'epi', 'epsilon', 'equally', 'er', 'ere', 'ericson', 'ericsson', 'erm', 'erode', 'erotic', 'err', 'error', 'erupt', 'erutupalam', 'esaplanade', 'escalator', 'escape', 'ese', 'eshxxxxxxxxxxx', 'espe', 'especially', 'esplanade', 'essay', 'essential', 'eta', 'etlp', 'ettans', 'euro', 'euro2004', 'eurodisinc', 'europe', 'evaporated', 'eve', 'eveb', 'evening', 'evenings', 'events', 'eventually', 'every1', 'everybody', 'everyboy', 'everyday', 'everyones', 'everyso', 'everythin', 'everytime', 'evey', 'eviction', 'evil', 'evn', 'evng', 'evo', 'evone', 'evr', 'evrey', 'evry', 'evry1', 'evrydy', 'ew', 'ex', 'exact', 'exactly', 'exam', 'exams', 'excellent', 'exchanged', 'excited', 'exciting', 'excuse', 'excused', 'excuses', 'exe', 'executive', 'exercise', 'exeter', 'exhaust', 'exhausted', 'exhibition', 'exist', 'exmpel', 'exorcism', 'exorcist', 'expect', 'expected', 'expecting', 'expects', 'expensive', 'experience', 'experiencehttp', 'experiment', 'expert', 'expired', 'expires', 'expiry', 'explain', 'explicit', 'explicitly', 'explosive', 'exposed', 'exposes', 'express', 'expression', 'expressoffer', 'extra', 'extract', 'extreme', 'ey', 'eye', 'eyed', 'eyes', 'f4q', 'fa', 'fab', 'faber', 'face', 'facebook', 'facilities', 'fact', 'factory', 'faded', 'faggot', 'faggy', 'faglord', 'failed', 'failing', 'fails', 'failure', 'fainting', 'fair', 'faith', 'fake', 'fakeye', 'fal', 'falconerf', 'fall', 'fallen', 'falling', 'falls', 'fals', 'famamus', 'familiar', 'family', 'fan', 'fancied', 'fancies', 'fancy', 'fantasies', 'fantastic', 'fantasy', 'far', 'farm', 'farrell', 'fast', 'faster', 'fastest', 'fat', 'fated', 'father', 'fathima', 'fats', 'fatty', 'fault', 'fav', 'fave', 'favor', 'favorite', 'favour', 'favourite', 'fb', 'fear', 'feathery', 'features', 'feb', 'february', 'fed', 'fedex', 'feed', 'feel', 'feelin', 'feeling', 'feelingood', 'feels', 'fees', 'feet', 'fell', 'fellow', 'felt', 'female', 'feng', 'festival', 'fetch', 'fever', 'ffectionate', 'fffff', 'ffffuuuuuuu', 'fgkslpo', 'fgkslpopw', 'fidalfication', 'field', 'fieldof', 'fiend', 'fifa', 'fight', 'fighting', 'fightng', 'fights', 'figure', 'figures', 'file', 'files', 'filled', 'filling', 'fills', 'film', 'films', 'filthy', 'filthyguys', 'final', 'finalise', 'finally', 'finance', 'financial', 'finding', 'fine', 'finest', 'fingers', 'finish', 'finishd', 'finished', 'finishing', 'fink', 'finn', 'fires', 'fish', 'fishhead', 'fishrman', 'fit', 'fiting', 'fix', 'fixed', 'fixedline', 'fixes', 'flag', 'flaked', 'flaky', 'flame', 'flash', 'flat', 'flatter', 'flavour', 'flea', 'flew', 'flies', 'flight', 'flights', 'flim', 'flip', 'flirt', 'flirting', 'flirtparty', 'floating', 'floor', 'floppy', 'florida', 'flow', 'flower', 'flowing', 'fluids', 'flurries', 'flute', 'fly', 'flying', 'flyng', 'fml', 'fo', 'fold', 'foley', 'follow', 'followed', 'followin', 'following', 'follows', 'fone', 'foned', 'fones', 'fonin', 'food', 'fool', 'fooled', 'fools', 'foot', 'football', 'footprints', 'footy', 'force', 'forced', 'foregate', 'foreign', 'forever', 'forevr', 'forfeit', 'forget', 'forgets', 'forgive', 'forgiven', 'forgiveness', 'forgot', 'forgotten', 'forgt', 'form', 'formal', 'formally', 'format', 'forms', 'fortune', 'forum', 'forums', 'forward', 'forwarded', 'foundurself', 'fourth', 'foward', 'fowler', 'fox', 'fps', 'fr', 'fran', 'frank', 'franxx', 'franyxxxxx', 'frauds', 'freak', 'freaked', 'freaking', 'free', 'free2day', 'freedom', 'freefone', 'freek', 'freemsg', 'freephone', 'freezing', 'fren', 'french', 'frens', 'frequently', 'fresh', 'freshers', 'fret', 'fri', 'friday', 'fridays', 'friend', 'friends', 'friendsare', 'friendship', 'friendships', 'fring', 'fringe', 'frm', 'frmcloud', 'frnd', 'frnds', 'frndship', 'frndshp', 'frndsship', 'frndz', 'frnt', 'fro', 'frog', 'fromm', 'fromwrk', 'frontierville', 'frosty', 'fruit', 'frwd', 'ft', 'fudge', 'fuelled', 'fujitsu', 'ful', 'fulfil', 'fullonsms', 'fumbling', 'fun', 'function', 'functions', 'fund', 'fundamentals', 'funeral', 'funk', 'funny', 'funs', 'fusion', 'future', 'fuuuuck', 'fwiw', 'fyi', 'g2', 'g696ga', 'ga', 'gage', 'gail', 'gain', 'gained', 'gal', 'galileo', 'gals', 'gam', 'gamb', 'game', 'games', 'gamestar', 'gandhipuram', 'ganesh', 'gang', 'gap', 'gaps', 'garage', 'garbage', 'gardener', 'gari', 'gary', 'gas', 'gastroenteritis', 'gate', 'gauge', 'gautham', 'gauti', 'gave', 'gay', 'gayle', 'gaytextbuddy', 'gaze', 'gbp', 'gbp4', 'gbp5', 'gd', 'ge', 'gee', 'geeee', 'geeeee', 'gei', 'gek1510', 'gender', 'generally', 'genes', 'genius', 'gent', 'gentle', 'gentleman', 'gently', 'genuine', 'genus', 'geoenvironmental', 'george', 'gep', 'ger', 'germany', 'get4an18th', 'getiing', 'geting', 'gets', 'getsleep', 'getstop', 'gettin', 'getting', 'getzed', 'gf', 'ghodbandar', 'ghost', 'gibbs', 'gibe', 'gift', 'gifted', 'gifts', 'gigolo', 'gimme', 'gin', 'girl', 'girlfrnd', 'girlie', 'girls', 'gist', 'giv', 'given', 'gives', 'giving', 'glad', 'glands', 'glasgow', 'glass', 'glo', 'glory', 'gloucesterroad', 'gm', 'gmw', 'gn', 'gnarls', 'gnt', 'gnun', 'go2sri', 'goal', 'goals', 'gobi', 'god', 'gods', 'goes', 'goggles', 'goigng', 'goin', 'going', 'gold', 'golddigger', 'golden', 'goldviking', 'golf', 'gon', 'gona', 'gone', 'gong', 'gonna', 'gonnamissu', 'good', 'goodevening', 'goodfriend', 'goodies', 'goodmate', 'goodmorning', 'goodness', 'goodnight', 'goodnite', 'goodnoon', 'goodo', 'goods', 'goodtime', 'google', 'gopalettan', 'gorgeous', 'gosh', 'gossip', 'got', 'gota', 'gotany', 'gotbabes', 'goto', 'gotta', 'gotten', 'gotto', 'goverment', 'govt', 'gower', 'gprs', 'gpu', 'gr8', 'gr8prizes', 'grab', 'grace', 'graduated', 'grahmbell', 'gram', 'grams', 'grand', 'grandfather', 'grandmas', 'granite', 'granted', 'graphics', 'grateful', 'grave', 'gravel', 'gravity', 'gravy', 'gray', 'grazed', 'gre', 'great', 'greatly', 'greatness', 'greece', 'green', 'greet', 'greeting', 'greetings', 'grief', 'grins', 'grinule', 'grl', 'grooved', 'groovy', 'groovying', 'ground', 'group', 'grow', 'growing', 'grown', 'grownup', 'grr', 'gsex', 'gsoh', 'gt', 'gua', 'guai', 'guaranteed', 'gud', 'gudni8', 'gudnite', 'gudnyt', 'guess', 'guesses', 'guessin', 'guessing', 'guidance', 'guide', 'guides', 'guilty', 'guitar', 'gumby', 'guoyang', 'gurl', 'gut', 'guy', 'guys', 'gv', 'gving', 'gym', 'gymnastics', 'gynae', 'gyno', 'ha', 'habba', 'habit', 'hadn', 'haf', 'haha', 'hahaha', 'hai', 'hail', 'hair', 'haircut', 'haiz', 'half', 'half8th', 'hall', 'halla', 'hallaq', 'halloween', 'ham', 'hamper', 'hamster', 'hand', 'handed', 'handing', 'handle', 'hands', 'handset', 'handsome', 'handsomes', 'hang', 'hanger', 'hangin', 'hanging', 'hannaford', 'hanuman', 'hanumanji', 'happen', 'happend', 'happened', 'happening', 'happens', 'happier', 'happiest', 'happily', 'happiness', 'happy', 'hard', 'hardcore', 'harder', 'hardly', 'harish', 'harri', 'harry', 'hasbro', 'hasn', 'hassling', 'hat', 'hate', 'hates', 'haughaighgtujhyguj', 'haunt', 'hav', 'hav2hear', 'hava', 'havbeen', 'havebeen', 'haven', 'havent', 'haventcn', 'having', 'havn', 'havnt', 'hcl', 'hdd', 'head', 'headache', 'headin', 'heading', 'heads', 'headset', 'headstart', 'heap', 'hear', 'heard', 'hearing', 'heart', 'hearts', 'heat', 'heater', 'heaven', 'heavily', 'heavy', 'hectic', 'hee', 'height', 'held', 'helen', 'helens', 'hell', 'hella', 'hello', 'hellogorgeous', 'help', 'help08700621170150p', 'help08714742804', 'helpful', 'helping', 'helpline', 'helps', 'hen', 'henry', 'hep', 'hero', 'heroes', 'heron', 'hes', 'hesitant', 'hesitate', 'hesitation', 'hex', 'hey', 'hf8', 'hg', 'hi', 'hidden', 'hide', 'hides', 'hiding', 'high', 'highest', 'hilarious', 'hill', 'hillsborough', 'himso', 'hint', 'hip', 'hiphop', 'hire', 'history', 'hit', 'hitler', 'hitman', 'hits', 'hitter', 'hittng', 'hiya', 'hl', 'hlp', 'hm', 'hme', 'hmm', 'hmmm', 'hmmmm', 'hmph', 'hmv', 'hmv1', 'ho', 'hockey', 'hogli', 'hogolo', 'hol', 'hold', 'holder', 'holding', 'hole', 'holiday', 'holla', 'hollalater', 'hols', 'holy', 'home', 'homeowners', 'hon', 'honestly', 'honey', 'honeybee', 'honeymoon', 'hont', 'hoo', 'hooch', 'hoody', 'hook', 'hooked', 'hoops', 'hop', 'hope', 'hoped', 'hopefully', 'hopeing', 'hopes', 'hopeso', 'hopeu', 'hoping', 'hor', 'horniest', 'horny', 'horo', 'horrible', 'hos', 'hospital', 'hospitals', 'host', 'hostel', 'hot', 'hotel', 'hotels', 'hotmail', 'hottest', 'hour', 'hourish', 'hours', 'house', 'houseful', 'housewives', 'housework', 'housing', 'howard', 'howda', 'howdy', 'hows', 'howu', 'howz', 'hp', 'hp20', 'hppnss', 'hr', 'hrishi', 'hrs', 'hsbc', 'html', 'http', 'hu', 'huai', 'hubby', 'hug', 'huge', 'hugging', 'hugs', 'huh', 'hui', 'huiming', 'hum', 'humanities', 'humans', 'hun', 'hundreds', 'hungover', 'hungry', 'hunks', 'hunny', 'hunting', 'hurricanes', 'hurried', 'hurry', 'hurt', 'hurting', 'hurts', 'husband', 'hut', 'hv', 'hv9d', 'hvae', 'hw', 'hyde', 'hype', 'hypertension', 'hypotheticalhuagauahahuagahyuhagga', 'iam', 'ias', 'ibh', 'ibhltd', 'ibiza', 'ibm', 'ibored', 'ibuprofens', 'ic', 'iccha', 'ice', 'icic', 'icicibank', 'icky', 'icmb3cktz8r7', 'id', 'idc', 'idea', 'ideal', 'ideas', 'identification', 'identifier', 'idew', 'idiot', 'idk', 'idps', 'idu', 'ifink', 'ig11', 'ignorant', 'ignore', 'ignoring', 'ikea', 'il', 'ileave', 'ill', 'illness', 'illspeak', 'ilol', 'im', 'image', 'imagination', 'imagine', 'imat', 'imf', 'img', 'imin', 'imma', 'immed', 'immediately', 'immunisation', 'imp', 'impatient', 'impede', 'implications', 'important', 'importantly', 'imposed', 'impossible', 'imposter', 'impress', 'impressed', 'impression', 'impressively', 'improve', 'improved', 'in2', 'inch', 'inches', 'incident', 'inclu', 'include', 'includes', 'including', 'inclusive', 'inconsiderate', 'inconvenience', 'inconvenient', 'incorrect', 'increase', 'incredible', 'inde', 'independence', 'india', 'indian', 'indians', 'individual', 'indyarocks', 'inever', 'infections', 'infernal', 'influx', 'info', 'inform', 'information', 'informed', 'infra', 'infront', 'ing', 'ingredients', 'ink', 'inlude', 'inner', 'innings', 'innocent', 'innu', 'inperialmusic', 'inpersonation', 'inr', 'insects', 'insha', 'inshah', 'inside', 'inspection', 'inst', 'install', 'installation', 'installing', 'instant', 'instantly', 'instead', 'instituitions', 'instructions', 'insurance', 'intelligent', 'intend', 'intention', 'interested', 'interesting', 'interfued', 'internal', 'internet', 'interview', 'interviews', 'interviw', 'intha', 'intrepid', 'intro', 'intrude', 'invaders', 'invention', 'invest', 'invitation', 'invite', 'invited', 'inviting', 'invnted', 'involve', 'iouri', 'ip4', 'ipad', 'ipaditan', 'iphone', 'ipod', 'iq', 'irene', 'iriver', 'ironing', 'irritated', 'irritates', 'irritating', 'irritation', 'isaiah', 'iscoming', 'ish', 'ishtamayoo', 'island', 'islands', 'isn', 'isnt', 'issue', 'issues', 'isv', 'italian', 'itcould', 'iter', 'ithink', 'itna', 'itried2tell', 'itwhichturnedinto', 'itxt', 'itz', 'ivatte', 'ive', 'iwana', 'iwas', 'iz', 'izzit', 'j5q', 'j89', 'jabo', 'jacket', 'jackpot', 'jackson', 'jacuzzi', 'jada', 'jade', 'jam', 'james', 'jamster', 'jamz', 'jan', 'janarige', 'jane', 'janinexx', 'january', 'jap', 'japanese', 'jas', 'jason', 'java', 'jay', 'jaya', 'jaykwon', 'jaz', 'jazz', 'jb', 'jd', 'je', 'jealous', 'jeans', 'jeetey', 'jelly', 'jen', 'jenny', 'jenxxx', 'jeremiah', 'jeri', 'jerk', 'jerry', 'jersey', 'jess', 'jesus', 'jet', 'jetton', 'jez', 'ji', 'jia', 'jiayin', 'jide', 'jiu', 'jjc', 'jo', 'joanna', 'job', 'jobs', 'jobyet', 'jocks', 'jod', 'jog', 'jogging', 'john', 'join', 'joined', 'joining', 'joke', 'joker', 'jokes', 'jokin', 'joking', 'jolly', 'jolt', 'jon', 'jones', 'jontin', 'jordan', 'jot', 'journey', 'joy', 'joys', 'jp', 'js', 'jsco', 'jst', 'jstfrnd', 'jsut', 'juan', 'judgemental', 'juicy', 'jules', 'juliana', 'julianaland', 'july', 'jump', 'jumpers', 'june', 'junna', 'jurong', 'jus', 'just', 'justbeen', 'justify', 'justthought', 'juswoke', 'juz', 'k52', 'k61', 'kaaj', 'kadeem', 'kaiez', 'kaila', 'kaitlyn', 'kalaachutaarama', 'kalainar', 'kalisidare', 'kallis', 'kama', 'kanagu', 'kane', 'kanji', 'kano', 'kappa', 'karaoke', 'karo', 'kate', 'katexxx', 'kath', 'kavalan', 'kay', 'kaypoh', 'kb', 'ke', 'keen', 'keeping', 'keeps', 'kegger', 'keluviri', 'ken', 'kent', 'kept', 'kerala', 'keralacircle', 'keris', 'kettoda', 'key', 'keypad', 'keys', 'keyword', 'kfc', 'kg', 'khelate', 'ki', 'kicchu', 'kick', 'kickboxing', 'kickoff', 'kicks', 'kid', 'kidding', 'kids', 'kidz', 'kiefer', 'kill', 'killing', 'kills', 'kilos', 'kind', 'kinda', 'kindly', 'king', 'kingdom', 'kintu', 'kiosk', 'kip', 'kisi', 'kiss', 'kisses', 'kissing', 'kit', 'kitty', 'kl341', 'knackered', 'knee', 'knees', 'knew', 'knickers', 'knock', 'knocking', 'know', 'knowing', 'knows', 'knw', 'ko', 'kochi', 'kodstini', 'kodthini', 'kolathupalayam', 'konw', 'korche', 'korean', 'korli', 'korte', 'kotees', 'kothi', 'kr', 'ktv', 'kuch', 'kudiyarasu', 'kusruthi', 'kvb', 'kz', 'l8', 'l8er', 'l8r', 'l8rs', 'l8tr', 'la', 'la1', 'la3', 'la32wu', 'lab', 'labor', 'lac', 'lacking', 'lacs', 'ladies', 'lady', 'lag', 'lage', 'lager', 'laid', 'lakhs', 'lambda', 'lambu', 'lamp', 'land', 'landing', 'landline', 'landlineonly', 'landlines', 'landmark', 'lands', 'lane', 'langport', 'language', 'lanka', 'lanre', 'lap', 'lapdancer', 'laptop', 'lar', 'laready', 'large', 'largest', 'lark', 'lastest', 'late', 'lately', 'latelyxxx', 'later', 'latest', 'latr', 'laugh', 'laughed', 'laughing', 'laughs', 'laundry', 'lautech', 'lavender', 'law', 'laxinorficated', 'lay', 'layin', 'lays', 'lazy', 'lccltd', 'ld', 'ldew', 'ldn', 'ldnw15h', 'le', 'lead', 'leadership', 'leading', 'leads', 'leaf', 'leafcutter', 'league', 'leanne', 'learn', 'learned', 'least5times', 'leave', 'leaves', 'leaving', 'lect', 'lecture', 'lecturer', 'left', 'leftovers', 'leg', 'legal', 'legitimat', 'legs', 'leh', 'lei', 'lemme', 'length', 'lengths', 'lennon', 'leo', 'leona', 'les', 'lesson', 'lessons', 'let', 'lets', 'letter', 'letters', 'level', 'lf56', 'li', 'liao', 'lib', 'libertines', 'library', 'lick', 'licks', 'lid', 'lido', 'lie', 'lies', 'life', 'lifebook', 'lifeis', 'lifetime', 'lifpartnr', 'lift', 'lifted', 'lifting', 'light', 'lik', 'like', 'liked', 'likely', 'likes', 'liking', 'lil', 'lily', 'lim', 'limiting', 'limits', 'limping', 'lindsay', 'line', 'linear', 'linerental', 'lines', 'lingo', 'link', 'links', 'linux', 'lion', 'lionm', 'lionp', 'lions', 'lip', 'lipo', 'lips', 'liquor', 'list', 'listen', 'listened2the', 'listening', 'listening2the', 'listn', 'lists', 'lit', 'literally', 'litres', 'little', 'live', 'lived', 'liver', 'liverpool', 'lives', 'living', 'lk', 'lkpobox177hp51fl', 'll', 'llspeak', 'lmao', 'lo', 'load', 'loads', 'loan', 'loans', 'lobby', 'local', 'location', 'locations', 'locaxx', 'lock', 'lodging', 'log', 'logged', 'logging', 'login', 'logo', 'logoff', 'logon', 'logos', 'loko', 'lol', 'lolnice', 'lololo', 'london', 'loneliness', 'lonely', 'long', 'longer', 'lonlines', 'loo', 'look', 'lookatme', 'looked', 'lookin', 'looking', 'looks', 'lool', 'loose', 'loosu', 'lor', 'lord', 'lose', 'losers', 'loses', 'losing', 'loss', 'lost', 'lot', 'lotr', 'lots', 'lotsof', 'lotta', 'lotto', 'lotz', 'loud', 'lounge', 'lousy', 'lov', 'lovable', 'love', 'loved', 'lovejen', 'lovely', 'loveme', 'lover', 'loverboy', 'lovers', 'loves', 'lovin', 'loving', 'lovingly', 'low', 'lower', 'lowes', 'loxahatchee', 'loyal', 'loyalty', 'lrg', 'ls1', 'ls15hb', 'ls278bb', 'lst', 'lt', 'ltdhelpdesk', 'lttrs', 'lubly', 'luck', 'luckily', 'lucky', 'lucozade', 'lucyxx', 'luks', 'lunch', 'lunsford', 'lush', 'luton', 'luv', 'luvd', 'luvs', 'lux', 'luxury', 'lv', 'lvblefrnd', 'lyf', 'lyfu', 'lyk', 'm100', 'm221bp', 'm227xy', 'm26', 'm263uz', 'm39m51', 'm6', 'm60', 'm8', 'm8s', 'm95', 'ma', 'maaaan', 'maangalyam', 'maat', 'mac', 'macedonia', 'macha', 'machan', 'machi', 'machines', 'macho', 'mack', 'macleran', 'macs', 'mad', 'madam', 'madodu', 'madoke', 'madstini', 'madurai', 'mag', 'maga', 'maggi', 'magical', 'mah', 'mahal', 'mahaveer', 'mahfuuz', 'maid', 'mail', 'mailbox', 'mailed', 'mails', 'main', 'maintain', 'major', 'make', 'makes', 'makiing', 'makin', 'making', 'malaria', 'malarky', 'male', 'mall', 'mallika', 'man', 'manage', 'manageable', 'managed', 'management', 'manchester', 'manda', 'mandan', 'mandara', 'maneesha', 'mango', 'maniac', 'manual', 'map', 'mapquest', 'maps', 'maraikara', 'marandratha', 'march', 'maretare', 'margaret', 'margin', 'marine', 'mark', 'market', 'marketing', 'marking', 'marley', 'marrge', 'marriage', 'married', 'marry', 'marsms', 'maruti', 'marvel', 'mas', 'masked', 'massage', 'massages', 'massive', 'masteriastering', 'masters', 'mat', 'match', 'matched', 'matches', 'mate', 'mates', 'math', 'mathe', 'mathematics', 'mathews', 'maths', 'matra', 'matric', 'matrix3', 'matter', 'matters', 'matthew', 'matured', 'maturity', 'max', 'max10mins', 'max6', 'maximize', 'maximum', 'mayb', 'maybe', 'mb', 'mca', 'mcat', 'mcr', 'meal', 'meals', 'mean', 'meaning', 'meaningful', 'means', 'meant', 'measure', 'meatballs', 'med', 'medical', 'medicine', 'meds', 'mee', 'meet', 'meetin', 'meeting', 'meets', 'meg', 'mega', 'meh', 'mei', 'meive', 'mel', 'melle', 'melnite', 'melody', 'melt', 'member', 'members', 'membership', 'memorable', 'memories', 'memory', 'men', 'mens', 'mental', 'mention', 'mentioned', 'mentionned', 'menu', 'meow', 'merely', 'merememberin', 'merry', 'mesages', 'mess', 'message', 'messaged', 'messages', 'messaging', 'messed', 'messenger', 'messy', 'met', 'method', 'mf', 'mfl', 'mgs', 'mi', 'mia', 'michael', 'mid', 'middle', 'midnight', 'mids', 'miiiiiiissssssssss', 'mileage', 'miles', 'milk', 'millers', 'milta', 'min', 'mina', 'minapn', 'mind', 'minded', 'mindset', 'mini', 'minimum', 'minmobsmore', 'minmobsmorelkpobox177hp51fl', 'minmoremobsemspobox45po139wa', 'minnaminunginte', 'minor', 'mins', 'mint', 'minus', 'minute', 'minutes', 'minuts', 'miracle', 'mirror', 'misbehaved', 'miserable', 'mising', 'misplaced', 'miss', 'misscall', 'missed', 'missin', 'missing', 'missionary', 'missions', 'misss', 'missunderstding', 'missy', 'mist', 'mistake', 'mistakes', 'misundrstud', 'mite', 'mitsake', 'miwa', 'mix', 'mj', 'mjzgroup', 'mk17', 'mk45', 'ml', 'mm', 'mmm', 'mmmm', 'mmmmm', 'mmmmmm', 'mmmmmmm', 'mmsto', 'mnth', 'mnths', 'mo', 'moan', 'mob', 'mobcudb', 'mobile', 'mobiles', 'mobilesdirect', 'mobilesvary', 'mobileupd8', 'mobno', 'mobs', 'mobsi', 'mobstorequiz10ppm', 'moby', 'mobypobox734ls27yf', 'mode', 'model', 'modl', 'module', 'modules', 'mofo', 'moji', 'mojibiola', 'mokka', 'molested', 'mom', 'moment', 'moments', 'moms', 'mon', 'monday', 'money', 'monkeespeople', 'monkey', 'monkeyaround', 'monkeys', 'mono', 'monoc', 'monos', 'month', 'monthly', 'monthlysubscription', 'months', 'mood', 'moon', 'moral', 'morefrmmob', 'morn', 'mornin', 'morning', 'mornings', 'morphine', 'morrow', 'moseley', 'mother', 'motivate', 'motivating', 'motive', 'motor', 'motorola', 'mountain', 'mountains', 'mouse', 'mouth', 'moves', 'movie', 'movies', 'movietrivia', 'moving', 'mp3', 'mquiz', 'mr', 'mre', 'mrng', 'mrt', 'ms', 'msg', 'msg150p', 'msging', 'msgrcvd', 'msgrcvdhg', 'msgs', 'msn', 'mt', 'mth', 'mths', 'mtmsg', 'mtmsg18', 'mtmsgrcvd18', 'mtnl', 'mu', 'muah', 'muchand', 'muchxxlove', 'mudyadhu', 'mufti', 'muht', 'multimedia', 'multis', 'mum', 'mumbai', 'mumhas', 'mummy', 'mums', 'mundhe', 'munsters', 'murali', 'murder', 'murdered', 'murderer', 'mush', 'mushy', 'music', 'musical', 'musicnews', 'musta', 'musthu', 'mustprovide', 'mutai', 'muz', 'mw', 'mycalls', 'mymoby', 'myparents', 'mys', 'myspace', 'mystery', 'n8', 'n9dx', 'na', 'nachos', 'nag', 'nagar', 'nah', 'nahi', 'naked', 'nalla', 'nalli', 'name1', 'name2', 'named', 'names', 'nammanna', 'namous', 'nanny', 'nap', 'narcotics', 'nasdaq', 'naseeb', 'nasty', 'nat', 'nat27081980', 'natalie', 'natalie2k9', 'natalja', 'national', 'nationwide', 'natural', 'nature', 'natwest', 'naughty', 'nauseous', 'nav', 'navigate', 'nb', 'nbme', 'nd', 'ndship', 'ne', 'near', 'nearby', 'nearer', 'nearly', 'necesity', 'necessarily', 'necessary', 'necessity', 'neck', 'necklace', 'ned', 'need', 'needa', 'needed', 'needing', 'needle', 'needs', 'neft', 'negative', 'neglect', 'neglet', 'neighbor', 'neighbour', 'nelson', 'nervous', 'neshanth', 'net', 'netcollex', 'netflix', 'nething', 'netun', 'netvision', 'network', 'networking', 'networks', 'neva', 'nevamind', 'nevering', 'new', 'neway', 'newest', 'newport', 'newquay', 'news', 'newscaster', 'newsletter', 'newspapers', 'nhs', 'ni8', 'nic', 'nice', 'nichols', 'nick', 'nicky', 'nigeria', 'nigh', 'night', 'nighters', 'nightnight', 'nights', 'nigpun', 'nigro', 'nike', 'nikiyu4', 'nimbomsons', 'nimya', 'ninish', 'nino', 'nipost', 'nit', 'nite', 'nitro', 'nitros', 'nitw', 'nitz', 'nmde', 'no1', 'noe', 'noi', 'noice', 'noise', 'noisy', 'nok', 'nokia', 'nokia6600', 'nokia6650', 'nokias', 'noline', 'noncomittal', 'nonetheless', 'nookii', 'noon', 'nooooooo', 'noooooooo', 'nope', 'nora', 'norcorp', 'nordstrom', 'norm', 'norm150p', 'normal', 'normally', 'north', 'northampton', 'nos', 'nose', 'nosh', 'nosy', 'note', 'notebook', 'notes', 'nothin', 'notice', 'notifications', 'notified', 'notixiquating', 'nottingham', 'noun', 'november', 'now1', 'nowadays', 'nr31', 'nri', 'nt', 'ntimate', 'ntt', 'ntwk', 'nuclear', 'nuerologist', 'num', 'number', 'numbers', 'nursery', 'nurungu', 'nus', 'nusstu', 'nuther', 'nutter', 'nver', 'nvm', 'nw', 'nxt', 'ny', 'nyc', 'nydc', 'nyt', 'nytho', 'nã', 'o2', 'o2fwd', 'oath', 'obey', 'oble', 'oblisingately', 'oblivious', 'obviously', 'occasion', 'occupied', 'occupy', 'occur', 'ocean', 'october', 'odalebeku', 'odi', 'ofcourse', 'offcampus', 'offdam', 'offense', 'offer', 'offered', 'offering', 'offers', 'office', 'officer', 'official', 'officially', 'offline', 'ofice', 'ofsi', 'ofstuff', 'oga', 'ogunrinde', 'oh', 'oi', 'oic', 'oil', 'oja', 'ok', 'okay', 'okday', 'okey', 'okie', 'okmail', 'okors', 'ola', 'olage', 'olave', 'olayiwola', 'old', 'oli', 'ollu', 'olol', 'olowoyey', 'olympics', 'omg', 'omw', 'onam', 'onbus', 'oncall', 'ondu', 'ones', 'oni', 'onion', 'online', 'onluy', 'onwards', 'onwords', 'ooh', 'oooh', 'oooooh', 'ooooooh', 'oops', 'open', 'opened', 'opener', 'openin', 'opening', 'openings', 'operate', 'operator', 'opinion', 'opinions', 'opponenter', 'opportunity', 'opposed', 'opposite', 'opps', 'opt', 'opted', 'option', 'optout', 'or2optout', 'or2stoptxt', 'oral', 'orange', 'oranges', 'orchard', 'order', 'ordered', 'ordinator', 'ore', 'oredi', 'oreo', 'oreos', 'orig', 'original', 'orno', 'ors', 'ortxt', 'oru', 'os', 'oscar', 'oso', 'otbox', 'othrs', 'othrwise', 'otside', 'ou', 'ouch', 'outage', 'outages', 'outbid', 'outdoors', 'outfit', 'outfor', 'outgoing', 'outl8r', 'outreach', 'outs', 'outside', 'outsomewhere', 'outstanding', 'outta', 'ovarian', 'oveable', 'overa', 'overdid', 'overdose', 'overheating', 'overtime', 'ovr', 'ovulate', 'ovulation', 'ow', 'owe', 'owed', 'owned', 'owns', 'owo', 'oxygen', 'oyster', 'oz', 'pa', 'pack', 'package', 'packing', 'packs', 'padhe', 'page', 'pages', 'pai', 'paid', 'pain', 'painful', 'paining', 'painting', 'pairs', 'pale', 'palm', 'panalam', 'panasonic', 'pandy', 'panic', 'panicks', 'panren', 'panther', 'panties', 'pants', 'pap', 'papa', 'paper', 'papers', 'paperwork', 'parachute', 'paragon', 'paragraphs', 'parantella', 'parchi', 'parco', 'parent', 'parents', 'paris', 'park', 'parked', 'parking', 'participate', 'particular', 'particularly', 'partner', 'partnership', 'parts', 'party', 'paru', 'pases', 'pass', 'passable', 'passed', 'passes', 'passion', 'passionate', 'passport', 'password', 'passwords', 'past', 'patent', 'path', 'pathaya', 'paths', 'patients', 'patrick', 'pattern', 'patty', 'paul', 'pause', 'pavanaputra', 'pax', 'pay', 'payasam', 'payback', 'payed2day', 'payee', 'paying', 'payment', 'payments', 'payoh', 'pc', 'pc1323', 'pdate_now', 'peace', 'peaceful', 'peak', 'pears', 'pee', 'peeps', 'pehle', 'pei', 'pence', 'pendent', 'pending', 'penis', 'people', 'peoples', 'percentages', 'perf', 'perfect', 'perform', 'performed', 'perfume', 'period', 'peripherals', 'permanent', 'permission', 'permissions', 'perpetual', 'persevered', 'persian', 'persolvo', 'person', 'personal', 'personality', 'personally', 'persons', 'perspective', 'perumbavoor', 'pesky', 'pete', 'petexxx', 'petey', 'petrol', 'pg', 'ph', 'pharmacy', 'phasing', 'phd', 'philosophical', 'philosophy', 'phne', 'phoenix', 'phone', 'phone750', 'phonebook', 'phoned', 'phones', 'phony', 'photo', 'photos', 'php', 'phrase', 'phyhcmk', 'piah', 'pic', 'pick', 'picked', 'picking', 'pickle', 'pics', 'picsfree1', 'picture', 'pictures', 'pie', 'piece', 'pieces', 'pig', 'piggy', 'pilates', 'pile', 'pimples', 'pin', 'pink', 'pints', 'pisces', 'piss', 'pissed', 'pity', 'pix', 'pizza', 'pl', 'place', 'placed', 'placement', 'places', 'plaid', 'plan', 'plane', 'planet', 'planettalkinstant', 'planned', 'planning', 'plans', 'plate', 'play', 'played', 'player', 'players', 'playin', 'playing', 'playng', 'plaza', 'pleasant', 'pleased', 'pleassssssseeeeee', 'pleasure', 'pleasured', 'plenty', 'plm', 'ploughing', 'pls', 'plumbers', 'plumbing', 'plural', 'plus', 'plyr', 'plz', 'pm', 'pmt', 'po', 'po19', 'pobox', 'pobox1', 'pobox12n146tf150p', 'pobox202', 'pobox334', 'pobox36504w45wq', 'pobox365o4w45wq', 'pobox45w2tg150p', 'pobox75ldns7', 'pobox84', 'poboxox36504w45wq', 'pocay', 'pocked', 'pocketbabe', 'pocy', 'pod', 'poem', 'poet', 'point', 'points', 'poker', 'poking', 'pole', 'police', 'politicians', 'polo', 'poly', 'poly3', 'polyc', 'polyh', 'polyph', 'polyphonic', 'polys', 'pongal', 'ponnungale', 'poo', 'pooja', 'pookie', 'pool', 'poop', 'poor', 'poorly', 'poortiyagi', 'pop', 'popcorn', 'popped', 'popping', 'porn', 'porridge', 'portal', 'portege', 'portions', 'pos', 'pose', 'posh', 'posible', 'position', 'positions', 'positive', 'possession', 'possessive', 'possessiveness', 'possible', 'post', 'postal', 'postcard', 'postcode', 'posted', 'posting', 'postponed', 'posts', 'potato', 'potential', 'potter', 'pouch', 'pound', 'pounded', 'pounds', 'poured', 'pours', 'pouts', 'power', 'powerful', 'poyyarikatur', 'ppl', 'pple', 'ppm', 'ppm150', 'prabha', 'practice', 'practicing', 'practicum', 'practising', 'praises', 'prakesh', 'praps', 'prasad', 'prasanth', 'praveesh', 'pray', 'prayers', 'praying', 'prayrs', 'pre', 'predict', 'predicte', 'predicting', 'predictive', 'prefer', 'preferably', 'prem', 'premarica', 'premier', 'premium', 'prepaid', 'prepare', 'prepared', 'prepayment', 'preponed', 'preschoolco', 'prescribed', 'prescripiton', 'prescription', 'presence', 'present', 'presents', 'president', 'presleys', 'press', 'pressies', 'pressure', 'prestige', 'pretend', 'pretsorginta', 'pretsovru', 'pretty', 'prevent', 'previews', 'previous', 'previously', 'prey', 'price', 'pride', 'priest', 'prin', 'prince', 'princes', 'princess', 'print', 'printed', 'printer', 'printing', 'prior', 'priority', 'priscilla', 'privacy', 'private', 'prix', 'priya', 'prize', 'prizeawaiting', 'prizes', 'prizeswith', 'pro', 'prob', 'probably', 'problem', 'problematic', 'problems', 'problms', 'problum', 'probpop', 'probs', 'probthat', 'process', 'processed', 'prods', 'products', 'prof', 'professional', 'professors', 'profile', 'profit', 'program', 'programs', 'progress', 'project', 'projects', 'prolly', 'prometazine', 'promise', 'promised', 'promises', 'promoting', 'promotion', 'promptly', 'prompts', 'prone', 'proof', 'proove', 'properly', 'property', 'propose', 'propsd', 'pros', 'prospects', 'protect', 'prove', 'proverb', 'provided', 'provider', 'providing', 'province', 'proze', 'prsn', 'ps', 'pshew', 'psp', 'psxtra', 'psychic', 'pt2', 'ptbo', 'pub', 'public', 'pull', 'pulling', 'pulls', 'pump', 'punch', 'punish', 'punishment', 'punj', 'punto', 'puppy', 'pura', 'purchase', 'purchases', 'purity', 'purple', 'purpose', 'purse', 'push', 'pushbutton', 'pushes', 'puts', 'puttin', 'putting', 'puzzeles', 'puzzles', 'qatar', 'qbank', 'qet', 'qing', 'qjkgighjjgcbl', 'qlynnbv', 'quality', 'quarter', 'que', 'queen', 'queries', 'ques', 'question', 'questioned', 'questions', 'quick', 'quickly', 'quiet', 'quit', 'quite', 'quiteamuzing', 'quitting', 'quiz', 'quizclub', 'quote', 'quoting', 'r836', 'racal', 'racing', 'radiator', 'radio', 'raed', 'rael', 'raglan', 'rahul', 'raiden', 'railway', 'rain', 'raining', 'raise', 'raised', 'raj', 'rajas', 'raji', 'rajini', 'rajitha', 'rajnikant', 'rakhesh', 'raksha', 'rally', 'ralphs', 'ramaduth', 'ramen', 'ran', 'random', 'randomlly', 'randomly', 'randy', 'rang', 'range', 'ranjith', 'ranju', 'raping', 'rate', 'rates', 'ratio', 'rats', 'raviyog', 'rawring', 'rayan', 'rayman', 'rays', 'rcb', 'rcd', 'rct', 'rcv', 'rcvd', 'rd', 'rdy', 'reach', 'reache', 'reached', 'reaching', 'reacting', 'reaction', 'read', 'readers', 'readiness', 'reading', 'ready', 'real', 'real1', 'realise', 'realised', 'realising', 'reality', 'realize', 'realized', 'realizes', 'really', 'reallyneed', 'realy', 'reapply', 'rearrange', 'reason', 'reasonable', 'reasons', 'reassurance', 'reassuring', 'rebel', 'reboot', 'rebooting', 'rec', 'recd', 'receipt', 'receipts', 'receive', 'receivea', 'received', 'receiving', 'recent', 'recently', 'reception', 'recession', 'recharge', 'recieve', 'reckon', 'recognise', 'recognises', 'record', 'recorded', 'recorder', 'records', 'recount', 'recovery', 'recycling', 'red', 'redeemable', 'redeemed', 'reduce', 'ree', 'ref', 'reference', 'references', 'referin', 'reffering', 'refilled', 'reflection', 'reflex', 'refreshed', 'refund', 'refunded', 'refused', 'reg', 'regard', 'regarding', 'regards', 'register', 'registered', 'registration', 'regret', 'regretted', 'regular', 'rejected', 'related', 'relation', 'relationship', 'relatives', 'relax', 'relaxing', 'released', 'reliant', 'relieved', 'religiously', 'relocate', 'reltnship', 'rem', 'remain', 'remains', 'remb', 'remember', 'remembered', 'rememberi', 'remembr', 'remembrs', 'remind', 'reminded', 'reminder', 'reminding', 'reminds', 'remixed', 'removal', 'remove', 'removed', 'rencontre', 'renewal', 'renewing', 'rent', 'rental', 'renting', 'rentl', 'rents', 'repair', 'repairs', 'repeat', 'repeating', 'repent', 'replace', 'replacement', 'replied', 'replies', 'reply', 'replying', 'replys150', 'report', 'reppurcussions', 'representative', 'republic', 'request', 'requests', 'require', 'requires', 'research', 'resent', 'reservations', 'reserve', 'reserved', 'reset', 'residency', 'resizing', 'reslove', 'resolution', 'resolved', 'resort', 'respect', 'respectful', 'responce', 'respond', 'responding', 'response', 'responsibilities', 'responsibility', 'rest', 'restaurant', 'restock', 'restocked', 'restrict', 'resubbing', 'resubmit', 'result', 'results', 'resume', 'resuming', 'retard', 'retrieve', 'return', 'returned', 'returning', 'returns', 'reunion', 'reveal', 'revealed', 'revealing', 'reverse', 'review', 'revision', 'reward', 'rewarding', 'rg21', 'rgds', 'rice', 'rich', 'riddance', 'ride', 'right', 'rightio', 'rightly', 'rights', 'riley', 'rimac', 'ring', 'ringing', 'rings', 'ringtone', 'ringtoneking', 'ringtones', 'rinu', 'rip', 'ripped', 'rise', 'risk', 'risks', 'rite', 'ritten', 'river', 'road', 'roads', 'roast', 'rob', 'robinson', 'robs', 'rock', 'rocks', 'rodds1', 'rodger', 'rofl', 'roger', 'role', 'roles', 'rolled', 'roller', 'romantic', 'romcapspam', 'ron', 'room', 'roomate', 'roommate', 'roommates', 'rooms', 'rose', 'roses', 'rough', 'round', 'rounds', 'route', 'row', 'rowdy', 'rows', 'royal', 'rpl', 'rply', 'rs', 'rstm', 'rt', 'rtf', 'rtm', 'rto', 'ru', 'rub', 'rude', 'rudi', 'rugby', 'ruin', 'ruining', 'rule', 'rules', 'rum', 'rumbling', 'rummer', 'rumour', 'run', 'running', 'runs', 'rupaul', 'rush', 'rushing', 'ruthful', 'rv', 'rvx', 'rwm', 'ryan', 'ryder', 's3xy', 's89', 'sabarish', 'sac', 'sack', 'sacrifice', 'sad', 'sae', 'saeed', 'safe', 'safely', 'safety', 'sagamu', 'said', 'sake', 'salad', 'salam', 'salary', 'sale', 'sales', 'salesman', 'salmon', 'salon', 'salt', 'sam', 'samachara', 'samantha', 'sambar', 'samus', 'sandiago', 'sane', 'sang', 'sankatmochan', 'sankranti', 'sao', 'sar', 'sarasota', 'sarcasm', 'sarcastic', 'sariyag', 'sary', 'sashimi', 'sat', 'satanic', 'sathy', 'sathya', 'satisfied', 'satisfy', 'satthen', 'saturday', 'saucy', 'sausage', 'savamob', 'save', 'saved', 'saves', 'savings', 'saw', 'say', 'sayhey', 'sayin', 'saying', 'says', 'sayy', 'sc', 'scallies', 'scammers', 'scarcasim', 'scared', 'scary', 'scenery', 'sch', 'schedule', 'school', 'schools', 'science', 'scold', 'scool', 'score', 'scores', 'scoring', 'scotch', 'scotland', 'scrappy', 'scratches', 'scratching', 'scream', 'screamed', 'screaming', 'screen', 'scrounge', 'scrumptious', 'sculpture', 'sd', 'sday', 'sdryb8i', 'se', 'sea', 'search', 'searching', 'season', 'seat', 'sec', 'second', 'secondary', 'seconds', 'secret', 'secretary', 'secretly', 'secrets', 'secs', 'section', 'sections', 'secure', 'secured', 'sed', 'seeing', 'seekers', 'seeking', 'seen', 'sef', 'seh', 'sehwag', 'select', 'selected', 'selection', 'self', 'selfindependence', 'selfish', 'selflessness', 'sell', 'selling', 'sells', 'sem', 'semester', 'semi', 'semiobscure', 'sen', 'send', 'sender', 'sending', 'sends', 'senor', 'sense', 'sensible', 'sensitive', 'sent', 'sentence', 'senthil', 'sentiment', 'seperated', 'sept', 'series', 'seriously', 'served', 'service', 'services', 'serving', 'servs', 'set', 'setting', 'settings', 'settle', 'settled', 'seven', 'seventeen', 'sex', 'sexiest', 'sextextuk', 'sexual', 'sexy', 'sexychat', 'sez', 'sg', 'sh', 'sha', 'shadow', 'shag', 'shah', 'shakara', 'shakespeare', 'shall', 'shame', 'shampain', 'shangela', 'shanghai', 'shanil', 'shaping', 'share', 'shared', 'sharing', 'shaved', 'shd', 'sheet', 'sheets', 'sheffield', 'shelf', 'shell', 'shelves', 'sherawat', 'shesil', 'shhhhh', 'shifad', 'shijas', 'shijutta', 'shinco', 'shindig', 'shining', 'shiny', 'ship', 'shipped', 'shipping', 'shirt', 'shirts', 'shivratri', 'shld', 'shldxxxx', 'shock', 'shocking', 'shola', 'shoot', 'shop', 'shoppin', 'shopping', 'shoranur', 'shore', 'short', 'shortage', 'shortcode', 'shorter', 'shortly', 'shorts', 'shot', 'shoul', 'shoulders', 'shouldn', 'shouted', 'shouting', 'shoving', 'shower', 'showered', 'showers', 'showing', 'showr', 'showrooms', 'shows', 'shracomorsglsuplt', 'shrek', 'shrink', 'shrub', 'shsex', 'shud', 'shudvetold', 'shuhui', 'shun', 'shut', 'shy', 'si', 'sib', 'sic', 'sick', 'sigh', 'sighs', 'sight', 'sign', 'significant', 'signin', 'signing', 'siguviri', 'silence', 'silent', 'silently', 'silver', 'sim', 'simonwatson5120', 'simple', 'simpler', 'simply', 'simpsons', 'simulate', 'sinco', 'sindu', 'sing', 'singing', 'single', 'singles', 'sip', 'sipix', 'sips', 'sir', 'sirji', 'sis', 'sister', 'sisters', 'sit', 'site', 'sitll', 'sitter', 'sittin', 'sitting', 'situation', 'siva', 'size', 'sized', 'sk3', 'sk38xh', 'skilgme', 'skillgame', 'skills', 'skinny', 'skins', 'skint', 'skip', 'skirt', 'sky', 'skye', 'skype', 'skyped', 'skyving', 'slaaaaave', 'slacking', 'slap', 'slave', 'sleep', 'sleepin', 'sleeping', 'sleepingwith', 'sleeps', 'sleepwell', 'sleepy', 'slept', 'slice', 'slide', 'sliding', 'slightly', 'slip', 'slippers', 'slippery', 'slo', 'slob', 'slots', 'slovely', 'slow', 'slower', 'slowing', 'slowly', 'slp', 'slurp', 'small', 'smaller', 'smart', 'smartcall', 'smarter', 'smash', 'smashed', 'smear', 'smell', 'smells', 'smeone', 'smidgin', 'smile', 'smiled', 'smiles', 'smiling', 'smith', 'smoke', 'smoked', 'smokes', 'smoking', 'sms', 'smsco', 'smsing', 'smsrewards', 'smsservices', 'smth', 'sn', 'snake', 'snap', 'snappy', 'snatch', 'snd', 'snogs', 'snoring', 'snot', 'snow', 'snowboarding', 'snowman', 'snuggles', 'sochte', 'social', 'sofa', 'soft', 'software', 'soil', 'soiree', 'sol', 'sold', 'solihull', 'solve', 'solved', 'some1', 'somebody', 'someday', 'someonone', 'someplace', 'somerset', 'sometext', 'somethin', 'somewheresomeone', 'somewhr', 'somtimes', 'sonetimes', 'song', 'songs', 'sony', 'sonyericsson', 'soo', 'soon', 'sooner', 'soonlots', 'sooo', 'soooo', 'sooooo', 'sophas', 'sore', 'sorrow', 'sorrows', 'sorry', 'sort', 'sorta', 'sorted', 'sorting', 'sorts', 'sory', 'soryda', 'sos', 'soul', 'sound', 'sounding', 'sounds', 'soundtrack', 'soup', 'source', 'sources', 'south', 'southern', 'soz', 'sozi', 'sp', 'space', 'spacebucks', 'spaces', 'spageddies', 'spain', 'spam', 'spanish', 'spare', 'spark', 'spatula', 'speak', 'speaking', 'special', 'speciale', 'specialisation', 'specialise', 'specially', 'specific', 'specify', 'speechless', 'speed', 'speedchat', 'speeding', 'spell', 'spelled', 'spelling', 'spend', 'spending', 'spent', 'sphosting', 'spice', 'spider', 'spile', 'spinout', 'spiral', 'spirit', 'spiritual', 'spjanuary', 'spk', 'spl', 'splash', 'splat', 'splendid', 'split', 'splleing', 'spoiled', 'spoilt', 'spoke', 'spoken', 'spontaneously', 'spook', 'spoon', 'spoons', 'sport', 'sportsx', 'spose', 'spot', 'spotty', 'spouse', 'sppok', 'spreadsheet', 'spree', 'spring', 'springs', 'sprint', 'sptv', 'spun', 'spys', 'sq825', 'squatting', 'squeeeeeze', 'squeezed', 'squid', 'srsly', 'sry', 'st', 'stability', 'stadium', 'staff', 'stage', 'stagwood', 'stalking', 'stamped', 'stamps', 'stand', 'standard', 'standing', 'stands', 'stapati', 'star', 'starer', 'staring', 'starring', 'stars', 'starshine', 'start', 'started', 'starting', 'starts', 'starve', 'starving', 'starwars3', 'stash', 'stated', 'statement', 'statements', 'station', 'status', 'stay', 'stayed', 'stayin', 'staying', 'stays', 'std', 'stdtxtrate', 'steak', 'steal', 'stealing', 'steam', 'steamboat', 'steed', 'steering', 'step', 'steps', 'stereo', 'stereophonics', 'sterling', 'sterm', 'steve', 'stewartsize', 'steyn', 'sticky', 'stil', 'stitch', 'stock', 'stocked', 'stockport', 'stolen', 'stomach', 'stomps', 'stone', 'stoners', 'stones', 'stool', 'stop', 'stop2stop', 'stopcost', 'stopcs', 'stopped', 'stops', 'stopsms', 'stoptxtstop', 'store', 'stores', 'stories', 'storm', 'storming', 'story', 'str', 'str8', 'straight', 'strange', 'stranger', 'stream', 'street', 'stress', 'stressed', 'stressful', 'stressfull', 'stretch', 'strict', 'strike', 'strings', 'strip', 'stripes', 'strips', 'strokes', 'strong', 'strt', 'strtd', 'struggling', 'sts', 'stu', 'stubborn', 'stuck', 'studdying', 'student', 'students', 'studies', 'studio', 'study', 'studying', 'studyn', 'stuff', 'stuff42moro', 'stuffed', 'stuffing', 'stuffs', 'stunning', 'stupid', 'style', 'styles', 'styling', 'stylish', 'sub', 'subject', 'subletting', 'submitted', 'submitting', 'subpoly', 'subs', 'subs16', 'subscribe6gbp', 'subscribed', 'subscriber', 'subscribers', 'subscription', 'subscriptions', 'subscriptn3gbp', 'subsequent', 'success', 'successful', 'successfully', 'sucker', 'suckers', 'sucks', 'sudden', 'sue', 'suffer', 'suffering', 'suffers', 'sufficient', 'sugababes', 'sugar', 'suggest', 'suggestion', 'suggestions', 'suite', 'suite342', 'suitemates', 'suits', 'sum', 'sum1', 'suman', 'summer', 'summers', 'summon', 'sumthin', 'sun', 'sun0819', 'sunday', 'sundayish', 'sunlight', 'sunny', 'sunoco', 'sunroof', 'sunshine', 'suntec', 'sup', 'super', 'superb', 'supervisor', 'suply', 'supose', 'supplies', 'supply', 'support', 'suppose', 'supposed', 'suprman', 'sura', 'sure', 'surely', 'surf', 'surfing', 'surname', 'surprise', 'surprised', 'surrender', 'survey', 'sutra', 'sux', 'suzy', 'svc', 'sw7', 'sw73ss', 'swan', 'swann', 'swap', 'swashbuckling', 'swat', 'swatch', 'sway', 'swayze', 'swear', 'sweater', 'sweatter', 'sweet', 'sweetest', 'sweetheart', 'sweetie', 'sweets', 'swell', 'swhrt', 'swimming', 'swing', 'swiss', 'switch', 'swollen', 'swoop', 'swt', 'swtheart', 'syd', 'syllabus', 'symbol', 'symptoms', 'synced', 'syria', 'syrup', 'systems', 't91', 'ta', 'table', 'tablet', 'tablets', 'tackle', 'tacos', 'tactful', 'tactless', 'tahan', 'tai', 'tait', 'takecare', 'taken', 'takes', 'takin', 'taking', 'talent', 'talents', 'talk', 'talkbut', 'talkin', 'talking', 'talks', 'tall', 'tallahassee', 'tallent', 'tamilnadu', 'tampa', 'tank', 'tantrum', 'tap', 'tape', 'tariffs', 'tarot', 'tarpon', 'taste', 'tat', 'tata', 'tats', 'tattoos', 'tau', 'taunton', 'taxi', 'taxless', 'taxt', 'taylor', 'tayseer', 'tb', 'tbs', 'tc', 'tcr', 'tcs', 'tddnewsletter', 'te', 'tea', 'teach', 'teacher', 'teachers', 'teaches', 'teaching', 'team', 'teams', 'tear', 'tears', 'tease', 'teasing', 'tech', 'technical', 'technologies', 'tee', 'teenager', 'teeth', 'teju', 'tel', 'telephonic', 'teletext', 'tell', 'telling', 'tellmiss', 'tells', 'telly', 'telphone', 'telugu', 'temales', 'temp', 'temple', 'tenants', 'tendencies', 'tenerife', 'tensed', 'tension', 'teresa', 'term', 'terminated', 'terms', 'termsapply', 'ternal', 'terrible', 'terrific', 'terror', 'tescos', 'tessy', 'test', 'testing', 'tests', 'texas', 'texd', 'text', 'text82228', 'textand', 'textbook', 'textbuddy', 'textcomp', 'texted', 'textin', 'texting', 'textoperator', 'texts', 'th', 'thandiyachu', 'thangam', 'thank', 'thanks', 'thanks2', 'thanksgiving', 'thanku', 'thankyou', 'thanx', 'thanx4', 'thasa', 'thats', 'the4th', 'theacusations', 'theater', 'theatre', 'thecd', 'thedailydraw', 'theirs', 'thekingshead', 'themes', 'themob', 'thenampet', 'theoretically', 'theory', 'theplace', 'thepub', 'theres', 'thesmszone', 'thet', 'thew', 'theyre', 'thgt', 'thia', 'thing', 'things', 'think', 'thinked', 'thinkin', 'thinking', 'thinks', 'thinkthis', 'thirunelvali', 'thk', 'thkin', 'thm', 'thnk', 'thnq', 'thnx', 'tho', 'thot', 'thou', 'thought', 'thoughts', 'thousands', 'thout', 'thread', 'threats', 'threw', 'throat', 'throw', 'throwin', 'throws', 'ths', 'tht', 'thts', 'thuglyfe', 'thurs', 'thursday', 'thx', 'thy', 'tick', 'ticket', 'tickets', 'tie', 'tiempo', 'tiger', 'tight', 'tightly', 'tigress', 'tihs', 'til', 'till', 'tim', 'time', 'times', 'timi', 'timin', 'timing', 'tiny', 'tip', 'tips', 'tired', 'tiring', 'tirunelvai', 'tirunelvali', 'tirupur', 'tis', 'tissco', 'title', 'titles', 'tiwary', 'tix', 'tiz', 'tke', 'tkls', 'tkts', 'tlk', 'tlp', 'tm', 'tming', 'tmorow', 'tmorrow', 'tmr', 'tmrw', 'tmw', 'tnc', 'tncs', 'toa', 'toaday', 'tobacco', 'tobed', 'tocall', 'toclaim', 'today', 'todays', 'todo', 'tog', 'tohar', 'toilet', 'tok', 'token', 'toking', 'tol', 'told', 'toledo', 'tolerance', 'tolerat', 'toll', 'tom', 'tomarrow', 'tome', 'tomeandsaid', 'tomo', 'tomorro', 'tomorrow', 'tone', 'tones', 'tones2u', 'tones2you', 'tonight', 'tonights', 'tonite', 'tons', 'tonsolitusaswell', 'took', 'tookplace', 'tool', 'toot', 'tooth', 'toothpaste', 'tootsie', 'topic', 'toplay', 'toppoly', 'tops', 'tor', 'torch', 'torrents', 'tortilla', 'torture', 'tosend', 'toshiba', 'toss', 'tot', 'total', 'totally', 'totes', 'touch', 'touched', 'tough', 'toughest', 'tour', 'town', 'toxic', 'toyota', 'tp', 'track', 'trackmarque', 'trade', 'traditions', 'traffic', 'train', 'trained', 'training', 'trainners', 'trains', 'tram', 'tranquility', 'transaction', 'transfer', 'transfered', 'transfr', 'transfred', 'transport', 'trauma', 'trav', 'travel', 'treacle', 'treadmill', 'treasure', 'treat', 'treated', 'treatin', 'treats', 'trebles', 'tree', 'trends', 'trial', 'tried', 'trip', 'triple', 'trishul', 'triumphed', 'trivia', 'tron', 'trouble', 'troubleshooting', 'trouser', 'truble', 'truck', 'true', 'truffles', 'truly', 'trust', 'truth', 'try', 'tryin', 'trying', 'ts', 'tsandcs', 'tscs', 'tscs08714740323', 'tscs087147403231winawk', 'tsunami', 'tsunamis', 'tt', 'ttyl', 'tues', 'tuesday', 'tui', 'tuition', 'tul', 'tulip', 'tunde', 'tune', 'tunji', 'turkeys', 'turn', 'turning', 'turns', 'tuth', 'tv', 'twat', 'twice', 'twiggs', 'twilight', 'twinks', 'twins', 'twittering', 'tx', 'txt', 'txt250', 'txt82228', 'txtauction', 'txtin', 'txting', 'txtno', 'txts', 'txtstar', 'txtstop', 'txttowin', 'txtx', 'tyler', 'type', 'types', 'tyrone', 'u2moro', 'u4', 'ubandu', 'ubi', 'ud', 'ugadi', 'ugh', 'ugo', 'uh', 'uhhhhrmm', 'ujhhhhhhh', 'uk', 'ukp', 'uks', 'ultimate', 'ultimately', 'ultimatum', 'umma', 'ummifying', 'ummma', 'ummmmmaah', 'unable', 'unbelievable', 'unbreakable', 'unclaimed', 'uncle', 'uncles', 'uncomfortable', 'unconscious', 'unconsciously', 'unconvinced', 'uncut', 'underdtand', 'understand', 'understanding', 'understood', 'underwear', 'undrstndng', 'unemployed', 'uneventful', 'unfortunately', 'unfortuntly', 'unhappiness', 'unhappy', 'uni', 'unicef', 'unintentional', 'unintentionally', 'unique', 'united', 'units', 'univ', 'university', 'unjalur', 'unkempt', 'unknown', 'unless', 'unlike', 'unlimited', 'unmits', 'unni', 'unredeemed', 'unsecured', 'unsold', 'unspoken', 'unsub', 'unsubscribe', 'unsubscribed', 'untamed', 'up4', 'upcharge', 'upd8', 'update', 'update_now', 'upgrade', 'upgrading', 'upgrdcentre', 'upload', 'uploaded', 'upping', 'ups', 'upset', 'upstairs', 'upto', 'uptown', 'ur', 'urawinner', 'ure', 'urfeeling', 'urgent', 'urgently', 'urgh', 'urgnt', 'urgoin', 'urgran', 'urination', 'url', 'urmom', 'urn', 'urself', 'usb', 'usc', 'use', 'used', 'useful', 'useless', 'user', 'uses', 'usf', 'usher', 'using', 'usmle', 'usps', 'usual', 'usually', 'uterus', 'utter', 'uup', 'uve', 'uwana', 'uwant', 'uworld', 'va', 'vaazhthukkal', 'vague', 'vaguely', 'vale', 'valentine', 'valentines', 'valid', 'valid12hrs', 'valuable', 'value', 'valued', 'values', 'vargu', 'various', 'varma', 'varunnathu', 'vary', 'vasai', 'vat', 'vatian', 'vava', 'vco', 'vday', 've', 'vegas', 'vegetables', 'veggie', 'vehicle', 'velachery', 'velly', 'velusamy', 'venugopal', 'verify', 'version', 'versus', 'vettam', 'vewy', 'vibrant', 'vibrate', 'vic', 'victoria', 'victors', 'vid', 'video', 'videochat', 'videophones', 'videos', 'videosound', 'videosounds', 'vijay', 'vijaykanth', 'vikky', 'villa', 'village', 'vinobanagar', 'violated', 'violence', 'violet', 'vip', 'vipclub4u', 'virgil', 'virgin', 'virgins', 'virtual', 'visionsms', 'visit', 'visiting', 'visitor', 'visitors', 'vital', 'vitamin', 'viva', 'vivek', 'vivekanand', 'vl', 'voda', 'vodafone', 'vodka', 'voice', 'voicemail', 'volcanoes', 'vomit', 'vomitin', 'vomiting', 'vote', 'voted', 'vouch4me', 'voucher', 'vouchers', 'vry', 'vth', 'vu', 'w1', 'w111wx', 'w14rg', 'w1a', 'w1j', 'w1j6hl', 'w1jhl', 'w4', 'w45wq', 'w8in', 'wa', 'wa14', 'waaaat', 'wad', 'wadebridge', 'wah', 'wahala', 'wahay', 'waheed', 'waheeda', 'wahleykkum', 'waht', 'wait', 'waited', 'waitin', 'waiting', 'wake', 'waking', 'wales', 'waliking', 'walk', 'walked', 'walkin', 'walking', 'wall', 'wallet', 'wallpaper', 'walls', 'walmart', 'wamma', 'wan', 'wan2', 'wana', 'wanna', 'wannatell', 'want', 'wanted', 'wanting', 'wants', 'wap', 'waqt', 'warm', 'warming', 'warned', 'warner', 'warning', 'warranty', 'warwick', 'wasn', 'wasnt', 'waste', 'wasted', 'wasting', 'wat', 'watch', 'watches', 'watchin', 'watching', 'watchng', 'water', 'watever', 'watevr', 'wating', 'watr', 'wats', 'watts', 'waves', 'way', 'way2sms', 'waz', 'wc1n', 'wc1n3xx', 'weak', 'weakness', 'weaknesses', 'weapon', 'wear', 'wearing', 'weaseling', 'weather', 'web', 'web2mobile', 'webadres', 'webeburnin', 'website', 'wed', 'weddin', 'wedding', 'wedlunch', 'wednesday', 'weds', 'wee', 'weed', 'week', 'weekend', 'weekends', 'weekly', 'weeks', 'weigh', 'weighed', 'weight', 'weird', 'weirdest', 'weirdo', 'weirdy', 'weiyi', 'welcome', 'welp', 'wen', 'wendy', 'wenever', 'went', 'wenwecan', 'wer', 'weren', 'werethe', 'wesley', 'wesleys', 'west', 'westlife', 'westonzoyland', 'westshore', 'wet', 'wewa', 'whats', 'whatsup', 'wheat', 'wheel', 'wheellock', 'whenevr', 'whens', 'whereare', 'wherevr', 'wherre', 'whilltake', 'whispers', 'white', 'whn', 'whore', 'whos', 'whr', 'wicked', 'wicket', 'wicklow', 'wid', 'wif', 'wife', 'wifi', 'wihtuot', 'wikipedia', 'wil', 'wild', 'wildest', 'willing', 'willpower', 'win', 'wind', 'window', 'windows', 'wine', 'wined', 'wings', 'wining', 'winner', 'winning', 'wins', 'winterstone', 'wipe', 'wipro', 'wire3', 'wisdom', 'wise', 'wish', 'wisheds', 'wishes', 'wishin', 'wishing', 'wishlist', 'wiskey', 'wit', 'withdraw', 'wither', 'witin', 'witot', 'witout', 'wiv', 'wk', 'wkend', 'wkent', 'wkg', 'wkly', 'wks', 'wlcome', 'wld', 'wn', 'wnevr', 'wnt', 'wo', 'woah', 'wocay', 'woke', 'woken', 'woman', 'womdarfull', 'women', 'won', 'wondar', 'wondarfull', 'wonder', 'wonderful', 'wondering', 'wonders', 'wont', 'woo', 'woods', 'woohoo', 'woot', 'woould', 'worc', 'word', 'words', 'work', 'workand', 'workin', 'working', 'workout', 'works', 'world', 'worlds', 'worms', 'worried', 'worries', 'worry', 'worse', 'worst', 'worth', 'wot', 'wotu', 'wotz', 'woul', 'woulda', 'wouldn', 'wounds', 'wow', 'wrc', 'wrench', 'wrenching', 'wright', 'write', 'writhing', 'wrk', 'wrkin', 'wrking', 'wrks', 'wrld', 'wrnog', 'wrong', 'wrongly', 'wrote', 'ws', 'wt', 'wtc', 'wtf', 'wth', 'wthout', 'wud', 'wudn', 'wuld', 'wuldnt', 'wun', 'www', 'wylie', 'x2', 'x29', 'x49', 'xafter', 'xam', 'xavier', 'xchat', 'xclusive', 'xin', 'xmas', 'xoxo', 'xuhui', 'xx', 'xxsp', 'xxuk', 'xxx', 'xxxmobilemovieclub', 'xxxx', 'xxxxx', 'xxxxxxxx', 'xxxxxxxxxxxxxx', 'xy', 'y87', 'ya', 'yah', 'yahoo', 'yalru', 'yam', 'yan', 'yar', 'yards', 'yavnt', 'yaxx', 'yaxxx', 'yay', 'yck', 'yeah', 'year', 'years', 'yeh', 'yelling', 'yellow', 'yelow', 'yen', 'yeovil', 'yep', 'yer', 'yes', 'yest', 'yesterday', 'yetty', 'yetunde', 'yhl', 'yi', 'yijue', 'ym', 'ymca', 'yo', 'yoga', 'yogasana', 'yor', 'youi', 'young', 'youre', 'yourinclusive', 'yourjob', 'youuuuu', 'yoville', 'yowifes', 'yoyyooo', 'yr', 'yrs', 'ystrday', 'ything', 'yummy', 'yun', 'yunny', 'yuo', 'yuou', 'yup', 'yupz', 'zaher', 'zealand', 'zebra', 'zed', 'zeros', 'zhong', 'zindgi', 'zoe', 'zogtorius', 'zoom', 'zouk', 'zyada', 'ãº1', 'éˆ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soibamb\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracted features are\")\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features Data Frame\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000pes</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>0207</th>\n",
       "      <th>02072069400</th>\n",
       "      <th>...</th>\n",
       "      <th>zeros</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>ãº1</th>\n",
       "      <th>éˆ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4441</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4442</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4443</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4444</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4445</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4446 rows × 7455 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       00  000  000pes  008704050406  0089  01223585236  0125698789   02  \\\n",
       "0     0.0  0.0     0.0           0.0   0.0          0.0         0.0  0.0   \n",
       "1     0.0  0.0     0.0           0.0   0.0          0.0         0.0  0.0   \n",
       "2     0.0  0.0     0.0           0.0   0.0          0.0         0.0  0.0   \n",
       "3     0.0  0.0     0.0           0.0   0.0          0.0         0.0  0.0   \n",
       "4     0.0  0.0     0.0           0.0   0.0          0.0         0.0  0.0   \n",
       "...   ...  ...     ...           ...   ...          ...         ...  ...   \n",
       "4441  0.0  0.0     0.0           0.0   0.0          0.0         0.0  0.0   \n",
       "4442  0.0  0.0     0.0           0.0   0.0          0.0         0.0  0.0   \n",
       "4443  0.0  0.0     0.0           0.0   0.0          0.0         0.0  0.0   \n",
       "4444  0.0  0.0     0.0           0.0   0.0          0.0         0.0  0.0   \n",
       "4445  0.0  0.0     0.0           0.0   0.0          0.0         0.0  0.0   \n",
       "\n",
       "      0207  02072069400  ...  zeros  zhong  zindgi  zoe  zogtorius  zoom  \\\n",
       "0      0.0          0.0  ...    0.0    0.0     0.0  0.0        0.0   0.0   \n",
       "1      0.0          0.0  ...    0.0    0.0     0.0  0.0        0.0   0.0   \n",
       "2      0.0          0.0  ...    0.0    0.0     0.0  0.0        0.0   0.0   \n",
       "3      0.0          0.0  ...    0.0    0.0     0.0  0.0        0.0   0.0   \n",
       "4      0.0          0.0  ...    0.0    0.0     0.0  0.0        0.0   0.0   \n",
       "...    ...          ...  ...    ...    ...     ...  ...        ...   ...   \n",
       "4441   0.0          0.0  ...    0.0    0.0     0.0  0.0        0.0   0.0   \n",
       "4442   0.0          0.0  ...    0.0    0.0     0.0  0.0        0.0   0.0   \n",
       "4443   0.0          0.0  ...    0.0    0.0     0.0  0.0        0.0   0.0   \n",
       "4444   0.0          0.0  ...    0.0    0.0     0.0  0.0        0.0   0.0   \n",
       "4445   0.0          0.0  ...    0.0    0.0     0.0  0.0        0.0   0.0   \n",
       "\n",
       "      zouk  zyada  ãº1   éˆ  \n",
       "0      0.0    0.0  0.0  0.0  \n",
       "1      0.0    0.0  0.0  0.0  \n",
       "2      0.0    0.0  0.0  0.0  \n",
       "3      0.0    0.0  0.0  0.0  \n",
       "4      0.0    0.0  0.0  0.0  \n",
       "...    ...    ...  ...  ...  \n",
       "4441   0.0    0.0  0.0  0.0  \n",
       "4442   0.0    0.0  0.0  0.0  \n",
       "4443   0.0    0.0  0.0  0.0  \n",
       "4444   0.0    0.0  0.0  0.0  \n",
       "4445   0.0    0.0  0.0  0.0  \n",
       "\n",
       "[4446 rows x 7455 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training features Data Frame\")\n",
    "pd.DataFrame(sms_train_tfidf,columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now the data is in the form of `samples x features`. Any predictive pipeline such as feature filtering can be done."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
